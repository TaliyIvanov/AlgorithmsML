{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rDIqqhNknVxf",
        "GWSrc9UN7yvh",
        "U_lpnRgsKudO",
        "rN0OYQfDhVvt",
        "X3H3LwOKbrkS",
        "fYG6m2cr16CO",
        "tforRUQa7iBH"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг №1 - Инициализация класса"
      ],
      "metadata": {
        "id": "rDIqqhNknVxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создайте класс с именем MyLogReg. Данный класс при инициализации должен принимать на вход два параметра:\n",
        "\n",
        "n_iter – количество шагов градиентного спуска.\n",
        "\n",
        "По-умолчанию: 10\n",
        "\n",
        "learning_rate – коэффициент скорости обучения градиентного спуска.\n",
        "\n",
        "По-умолчанию: 0.1\n",
        "\n",
        "Все переданные (или дефолтные) параметры должны быть сохранены внутри класса.\n",
        "\n",
        "При обращении к экземпляру класса (или при передачи его в функцию print) необходимо распечатать строку по следующему шаблону:\n",
        "\n",
        "MyLogReg class: n_iter=n_iter,\n",
        "\n",
        "learning_rate=learning_rate"
      ],
      "metadata": {
        "id": "pzbkX2XOnbIr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xq1ixjEtnQJX"
      },
      "outputs": [],
      "source": [
        "class MyLogReg():\n",
        "\n",
        "  def __init__(self, n_iter=10, learning_rate=0.1):\n",
        "    self.n_iter = n_iter\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "  def __str__(self):\n",
        "    params = ', '.join(f'{key}={value}' for key, value in self.__dict__.items())\n",
        "    return f'{__class__.__name__} class: {params}'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(MyLogReg())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6fkCvuEoYYY",
        "outputId": "ccd4026b-5af3-4899-8a9a-2938044af63c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyLogReg class: n_iter=10, learning_rate=0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг №2 - Метод .fit()"
      ],
      "metadata": {
        "id": "GWSrc9UN7yvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализуем обучение нашей модели:\n",
        "\n",
        "В инициализатор класса добавить новый параметр – weights – который будет хранить веса модели. По умолчанию он ничего не содержит.\n",
        "Вам необходимо реализовать метод fit в Вашем классе. Данный метод должен делать следующее:\n",
        "На вход принимать три атрибута:\n",
        "- X – все фичи в виде датафрейма пандаса.\n",
        "  Примечание: даже если фича будет всего одна, это все равно будет датафрейм, а не серия.\n",
        "- y – целевая переменная в виде пандасовской серии.\n",
        "- verbose – указывает, на какой итерации выводить лог. Например, значение 10 означает, что на каждой 10 итерации градиентного спуска будет печататься лог. Значение по умолчанию = False (т.е. ничего не выводится).\n",
        "Дополнить переданную матрицу фичей единичным столбцом слева.\n",
        "Определить, сколько фичей передано и создать вектор весов, состоящий из одних единиц соответствующей длинны: т.е. количество фичей + 1.\n",
        "Дальше в цикле (до n_iter):\n",
        "- Предсказать\n",
        "y\n",
        "^\n",
        "y\n",
        "^\n",
        "​\n",
        " .\n",
        "- Посчитать ошибку (LogLoss).\n",
        "- Вычислить градиент.\n",
        "- Сделать шаг размером learning rate в противоположную от градиента сторону.\n",
        "- Сохранить обновленные веса внутри класса.\n",
        "В процессе обучения необходимо выводить лог, в котором указывать номер итерации и значение функций потерь:\n",
        "start | loss: 42027.65\n",
        "100 | loss: 1222.87\n",
        "200 | loss: 232.17\n",
        "300 | loss: 202.4\n",
        "где start - значении функции потерь до начала обучения. Далее выводится каждое i-ое значение итерации, переданное в параметре verbose. Если verbose = False, то лог не выводится вовсе.\n",
        "Метод ничего не возвращает.\n",
        "Необходимо реализовать метод get_coef, который будет возвращать значения весов в виде массива NumPy."
      ],
      "metadata": {
        "id": "I5VuydwF8K1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class MyLogReg():\n",
        "  def __init__(self, n_iter=10, learning_rate=0.1, weights=None):\n",
        "    self.n_iter = n_iter\n",
        "    self.learning_rate = learning_rate\n",
        "    self.weights = weights\n",
        "\n",
        "  def __str__(self):\n",
        "    params = ', '.join(f'{key}={value}' for key, value in self.__dict__.items())\n",
        "    return f'{__class__.__name__} class: {params}'\n",
        "\n",
        "  def fit(self, X:pd.DataFrame, y:pd.Series, verbose=False):\n",
        "    # дополняем матрицу фичей единичным столбцом слева\n",
        "    # это необходимо для перемножение первоначальных значений весов\n",
        "    X = np.hstack([np.ones((X.shape[0], 1)), X.values])\n",
        "\n",
        "    # инициализируем веса\n",
        "    self.weights = np.ones(X.shape[1])\n",
        "\n",
        "    # вычисляем начальное значение функции потерь\n",
        "    for i in range(self.n_iter):\n",
        "      # предсказания\n",
        "      y_pred =  1 / (1 + np.exp(-X.dot(self.weights)))  # логистическая функция\n",
        "      # считаем LogLoss\n",
        "      log_loss = -np.mean(y*np.log(y_pred + 1e-15) + (1-y) * np.log(1 - y_pred + 1e-15))\n",
        "      # вычисление градиента (частное производно)\n",
        "      gradient = np.dot(X.T, (y_pred-y)) / y.size\n",
        "      # обновление весов (шаг в противолодожную от градиента сторону)\n",
        "      self.weights -= self.learning_rate * gradient\n",
        "      if verbose and i % verbose == 0:\n",
        "        print(f'{i} | loss: {log_loss}')\n",
        "\n",
        "\n",
        "  def get_coef(self):\n",
        "    # Возвращаем все веса, начиная с первого (пропускаем фиктивный признак)\n",
        "    return self.weights[1:]"
      ],
      "metadata": {
        "id": "UvsalDDhob3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг №3 - Prediction"
      ],
      "metadata": {
        "id": "U_lpnRgsKudO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Научим модель выдавать предсказания. Добавьте в класс `MyLogReg` два метода `predict` и `predict_proba`. Оба метода должны делать следующее:\n",
        "\n",
        "1. На вход принимать матрицу фичей в виде датафрейма pandas.\n",
        "2. Дополнять матрицу фичей единичным вектором (первый столбец).\n",
        "3. Возвращать вектор предсказаний с одним отличием:\n",
        "   - `predict_proba` — возвращает вероятности (логиты, прогнанные через функцию сигмоиды).\n",
        "   - `predict` — переводит вероятности в бинарные классы по порогу > 0.5.\n",
        "\n",
        "Формула для предсказания вероятностей выглядит так:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\frac{1}{1 + e^{-XW}}\n",
        "$$\n",
        "\n",
        "где:\n",
        "\n",
        "- $X$ — матрица фичей;\n",
        "- $W$ — вектор весов.\n"
      ],
      "metadata": {
        "id": "iaWAgwb3KvN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyLogReg():\n",
        "  def __init__(self, n_iter=10, learning_rate=0.1, weights=None):\n",
        "    self.n_iter = n_iter\n",
        "    self.learning_rate = learning_rate\n",
        "    self.weights = weights\n",
        "\n",
        "  def __str__(self):\n",
        "    params = ', '.join(f'{key}={value}' for key, value in self.__dict__.items())\n",
        "    return f'{__class__.__name__} class: {params}'\n",
        "\n",
        "  def fit(self, X:pd.DataFrame, y:pd.Series, verbose=False):\n",
        "    # дополняем матрицу фичей единичным столбцом слева\n",
        "    # это необходимо для перемножения первоначальных значений весов\n",
        "    X = np.hstack([np.ones((X.shape[0], 1)), X.values])\n",
        "    # инициализируем веса\n",
        "    self.weights = np.ones(X.shape[1])\n",
        "    # вычисляем начальное значение функции потерь\n",
        "    for i in range(self.n_iter):\n",
        "      # предсказания\n",
        "      y_pred = 1 / (1 + np.exp(-X.dot(self.weights))) # логистическая ф-ия\n",
        "      # считаем LogLoss\n",
        "      log_loss = -np.mean(y * np.log(y_pred + 1e-9) + (1-y) * np.log(1 - y_pred + 1e-9))\n",
        "      # вычисление градиента (частной производной функции потерь)\n",
        "      gradient = np.dot(X.T, (y_pred-y)) / y.size\n",
        "      # обновление весов (шаг в противоположную от градиента сторону)\n",
        "      self.weights -= self.learning_rate * gradient\n",
        "      if verbose and i % verbose == 0:\n",
        "        print( f'{i} | loss: {log_loss}')\n",
        "\n",
        "  def get_coef(self):\n",
        "    return self.weights[1:]\n",
        "\n",
        "  def predict_proba(self, X:pd.DataFrame):\n",
        "    # дополняем матрицу фичей единичным столбцом слева\n",
        "    X = np.hstack([np.ones((X.shape[0], 1)), X.values])\n",
        "    return 1 / (1 + np.exp(-X.dot(self.weights)))\n",
        "\n",
        "  def predict(self, X:pd.DataFrame):\n",
        "    # дополняем матрицу фичей единичным столбцом слева\n",
        "    X = np.hstack([np.ones((X.shape[0], 1)), X.values])\n",
        "    return (1 / (1 + np.exp(-X.dot(self.weights))) > 0.5).astype(int)"
      ],
      "metadata": {
        "id": "iH7dZk8SKu2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг №4 - Метрики качества"
      ],
      "metadata": {
        "id": "rN0OYQfDhVvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализация метрик в классе MyLogReg\n",
        "\n",
        "Теперь реализуем метрики на практике:\n",
        "\n",
        "Добавьте в класс `MyLogReg` параметр `metric`, который будет принимать одно из следующих значений:\n",
        "- `accuracy`\n",
        "- `precision`\n",
        "- `recall`\n",
        "- `f1`\n",
        "- `roc_auc`\n",
        "\n",
        "По умолчанию: `None`.\n",
        "\n",
        "При обучении добавьте в вывод расчет метрики:\n",
        "\n",
        "start | loss: 42027.65 | metric_name: 234.65\n",
        "\n",
        "100 | loss: 1222.87 | metric_name: 114.35\n",
        "\n",
        "200 | loss: 232.17 | metric_name: 58.2\n",
        "\n",
        "300 | loss: 202.4 | metric_name: 46.01\n",
        "\n",
        "\n",
        "Если метрика не задана, то ничего дополнительно выводить не нужно.\n",
        "\n",
        "Добавьте метод `get_best_score`, который возвращает значение метрики уже обученной модели.\n",
        "\n",
        "Примечание\n",
        "\n",
        "Метрика ROC AUC сильно зависит от порядка следования объектов. А разная последовательность арифметических операций может привести к тому, что скоры в последних знаках после запятой могут отличаться. Что, в свою очередь, приведет к разной сортировке объектов и разной оценке ROC AUC. Чтобы этого избежать, округлите скоры при вычислении метрики ROC AUC до 10 знака после запятой. При этом скоры, возвращаемые моделью, должны остаться в неизменном виде."
      ],
      "metadata": {
        "id": "EB2A0oBmhVsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MySolution"
      ],
      "metadata": {
        "id": "AgRIpY3ZZqD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "class MyLogReg():\n",
        "  def __init__(self, n_iter=100, learning_rate=0.1, weights=None, metric=None):\n",
        "    self.n_iter = n_iter\n",
        "    self.learning_rate = learning_rate\n",
        "    self.weights = weights\n",
        "    self.metric = metric\n",
        "    self.best_score = None\n",
        "\n",
        "  def __str__(self):\n",
        "    params = ', '.join(f'{key}={value}' for key, value in self.__dict__.items())\n",
        "    return f'{__class__.__name__} class: {params}'\n",
        "\n",
        "  def compute_metric(self, y, y_pred):\n",
        "    y_pred_binary = (y_pred >= 0.5).astype(int)\n",
        "    # accuracy\n",
        "    if self.metric == 'accuracy':\n",
        "      return accuracy_score(y, y_pred_binary)\n",
        "    # precision\n",
        "    elif self.metric == 'precision':\n",
        "      return precision_score(y, y_pred_binary)\n",
        "    # recall\n",
        "    elif self.metric == 'recall':\n",
        "      return recall_score(y, y_pred_binary)\n",
        "    # f1\n",
        "    elif self.metric == 'f1':\n",
        "      return f1_score(y, y_pred_binary)\n",
        "    # roc_auc\n",
        "    elif self.metric == 'roc_auc':\n",
        "      # округляем предсказания до 10 знаков после запятой только для ROC-AUC\n",
        "      y_pred_rounded = np.round(y_pred, 10)\n",
        "      return roc_auc_score(y, y_pred_rounded)\n",
        "\n",
        "  def fit(self, X:pd.DataFrame, y:pd.Series, verbose=False):\n",
        "    # дополняем матрицу фичей единичным столбцом слева\n",
        "    # это необходимо для перемножения первоначальных значений весов\n",
        "    X = np.hstack([np.ones((X.shape[0], 1)), X.values])\n",
        "    # инициализируем веса\n",
        "    self.weights = np.ones(X.shape[1])\n",
        "    # вычисляем начальное значение функции потерь\n",
        "    for i in range(self.n_iter):\n",
        "      y_pred = 1 / (1 + np.exp(-X.dot(self.weights)))\n",
        "      # считаем logloss\n",
        "      log_loss = -np.mean(y * np.log(y_pred + 1e-15) + (1-y) * np.log(1 - y_pred + 1e-15))\n",
        "      # вычисление градиента (частной производной функции потерь)\n",
        "      gradient = np.dot(X.T, (y_pred-y)) / y.size\n",
        "      # обновление весов (шаг в противоположную сторону от градиента)\n",
        "      self.weights -= self.learning_rate * gradient\n",
        "      # Логирование на каждой итерации с отображением метрики\n",
        "      if verbose and i % verbose == 0:\n",
        "        metric_value = self.compute_metric(y, y_pred)\n",
        "        if self.metric:\n",
        "          print(f'i | loss: {loss:.2f} | {self.metric}: {metric_value:.2f}')\n",
        "        else:\n",
        "          print( f'{i} | loss: {log_loss}')\n",
        "    # Вычисляем метрику на последнм шаге после завершения всеъ итераций\n",
        "    y_pred_final = 1 / (1 + np.exp(-X.dot(self.weights)))  # Используем логистическую функцию\n",
        "    self.best_score = round(self.compute_metric(y, y_pred_final), 10)\n",
        "\n",
        "  def get_coef(self):\n",
        "    return self.weights[1:]\n",
        "\n",
        "  def predict_proba(self, X:pd.DataFrame):\n",
        "    X = np.hstack([np.ones((X.shape[0],1)), X.values])\n",
        "    return 1 / (1 +np.exp(-X.dot(self.weights)))\n",
        "\n",
        "  def predict(self, X:pd.DataFrame):\n",
        "    X = np.hstack([np.ones((X.shape[0],1)), X.values])\n",
        "    return (1 / (1 +np.exp(-X.dot(self.weights))) > 0.5).astype(int)\n",
        "\n",
        "  def get_best_score(self):\n",
        "    return self.best_score"
      ],
      "metadata": {
        "id": "uYQildOghWXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чье то решение с курсов, взял, чтобы в будущем разобраться с расчетом метрик"
      ],
      "metadata": {
        "id": "mBz1MihVZsKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TableMetrics:\n",
        "    def __init__(self, y_true, y_pred_proba, metric):\n",
        "        self.y_true = np.array(y_true)\n",
        "        self.y_pred = np.array((y_pred_proba > 0.5).astype(int))\n",
        "        self.y_pred_proba = y_pred_proba\n",
        "        self.tp = np.sum((self.y_true == 1) & (self.y_pred == 1))\n",
        "        self.fp = np.sum((self.y_true == 0) & (self.y_pred == 1))\n",
        "        self.fn = np.sum((self.y_true == 1) & (self.y_pred == 0))\n",
        "        self.tn = np.sum((self.y_true == 0) & (self.y_pred == 0))\n",
        "        self.metric = metric\n",
        "\n",
        "    def score(self):\n",
        "        if self.metric == 'accuracy':\n",
        "            return self.accuracy()\n",
        "        elif self.metric == 'precision':\n",
        "            return self.precision()\n",
        "        elif self.metric == 'recall':\n",
        "            return self.recall()\n",
        "        elif self.metric == 'f1':\n",
        "            return self.f1_score()\n",
        "        elif self.metric == 'roc_auc':\n",
        "            return self.roc_auc()\n",
        "        elif self.metric == 'false_positive_rate':\n",
        "            return self.false_positive_rate()\n",
        "        return None\n",
        "\n",
        "    def accuracy(self):\n",
        "        if self.tp + self.tn + self.fp + self.fn == 0:\n",
        "            return 0\n",
        "        return (self.tp + self.tn) / (self.tp + self.tn + self.fp + self.fn)\n",
        "\n",
        "    def precision(self):\n",
        "        if self.tp + self.fp == 0:\n",
        "            return 0\n",
        "        return self.tp / (self.tp + self.fp)\n",
        "\n",
        "    def recall(self):\n",
        "        if self.tp + self.fn == 0:\n",
        "            return 0\n",
        "        return self.tp / (self.tp + self.fn)\n",
        "\n",
        "    def false_positive_rate(self):\n",
        "        return self.fp / (self.fp + self.tn)\n",
        "\n",
        "    def f1_score(self):\n",
        "        precision = self.precision()\n",
        "        recall = self.recall()\n",
        "        if precision + recall == 0:\n",
        "            return 0\n",
        "        return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    def roc_auc(self):\n",
        "        sqr = 0\n",
        "        n_ones = np.sum(self.y_true == 1)\n",
        "        n_zeroes = np.sum(self.y_true == 0)\n",
        "        m = n_ones * n_zeroes\n",
        "        trip = sorted(zip(self.y_pred_proba, self.y_true), reverse=True)\n",
        "        for _, true in trip:\n",
        "            if true == 1:\n",
        "                sqr += n_zeroes\n",
        "            else:\n",
        "                n_zeroes -= 1\n",
        "        return sqr / m\n",
        "\n",
        "class MyLogReg:\n",
        "    def __init__(self, n_iter=10, learning_rate=0.1, metric=None):\n",
        "        self.n_iter = n_iter\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = None\n",
        "        self.metric = metric\n",
        "\n",
        "    def fit(self, X, y, verbose=False):\n",
        "        n_samples, n_features = X.shape\n",
        "        ones = np.ones((n_samples, 1))\n",
        "        X = np.hstack((ones, X))\n",
        "\n",
        "        self.weights = np.ones((n_features + 1, 1))\n",
        "\n",
        "        EPS = 1e-15\n",
        "        for iter in range(self.n_iter + 1):\n",
        "            z = X @ self.weights\n",
        "            y_pred = self.sigmoid(z).flatten()\n",
        "            logloss = -np.mean(y * np.log(y_pred + EPS) - (1 - y) * np.log(1 - y_pred + EPS))\n",
        "            grad = ((y_pred - y) @ X) / n_samples\n",
        "            self.weights -= self.learning_rate * grad.reshape(-1, 1)\n",
        "\n",
        "            self.scores = TableMetrics(y, y_pred, self.metric)\n",
        "\n",
        "            if verbose and iter % verbose == 0:\n",
        "                print(f\"{iter if iter != 0 else 'start'} | loss: {logloss}\", f\"| {self.metric}: {self.scores.score()}\" if self.metric else '')\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        n_samples, _ = X.shape\n",
        "        ones = np.ones((n_samples, 1))\n",
        "        X = np.hstack((ones, X))\n",
        "        return self.sigmoid(X @ self.weights)\n",
        "\n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        return (proba > 0.5).astype(int)\n",
        "\n",
        "    def get_coef(self):\n",
        "        return self.weights[1:]\n",
        "\n",
        "    def get_best_score(self):\n",
        "        self.scores.roc_auc()\n",
        "        return self.scores.score()\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        z = np.clip(z, -250, 250)\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"MyLogReg class: n_iter={self.n_iter}, learning_rate={self.learning_rate}\""
      ],
      "metadata": {
        "id": "ndLYzbBdZwjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг №5 - Регуляризация"
      ],
      "metadata": {
        "id": "X3H3LwOKbrkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Регуляризация\n",
        "\n",
        "Теперь повторим регуляризацию для логистической регрессии. Она ничем не отличается от линейной — просто добавляем веса к функции потерь:\n",
        "\n",
        "L1 регуляризация (или Lasso регрессия)\n",
        "\n",
        "Добавляем модуль суммы весов к функции потерь:\n",
        "\n",
        "$$\n",
        "LassoLogLoss = -\\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right) + \\lambda_1 \\sum_{j=1}^{m} |w_j|\n",
        "$$\n",
        "\n",
        "Градиент:\n",
        "\n",
        "$$\n",
        "\\nabla (LassoLogLoss) = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_m \\end{bmatrix} + \\lambda_1 \\begin{bmatrix} \\text{sgn}(w_0) \\\\ \\text{sgn}(w_1) \\\\ \\vdots \\\\ \\text{sgn}(w_m) \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "или в виде матричного перемножения:\n",
        "\n",
        "$$\n",
        "\\nabla (LassoLogLoss) = \\frac{1}{n} (\\hat{Y} - Y) X + \\lambda_1 \\, \\text{sgn}(W)\n",
        "$$\n",
        "\n",
        "L2 регуляризация (или Ridge регрессия)\n",
        "\n",
        "Добавляем квадрат весов к функции потерь:\n",
        "\n",
        "$$\n",
        "RidgeLogLoss = -\\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right) + \\lambda_2 \\sum_{j=1}^{m} w_j^2\n",
        "$$\n",
        "\n",
        "Градиент:\n",
        "\n",
        "$$\n",
        "\\nabla (RidgeLogLoss) = \\frac{1}{n} (\\hat{Y} - Y) X + 2 \\lambda_2 W\n",
        "$$\n",
        "\n",
        "ElasticNet\n",
        "\n",
        "Добавляем и квадра и модуль весов:\n",
        "\n",
        "$$\n",
        "ElasticNetLogLoss = -\\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right) + \\lambda_1 \\sum_{j=1}^{m} |w_j| + \\lambda_2 \\sum_{j=1}^{m} w_j^2\n",
        "$$\n",
        "\n",
        "Градиент:\n",
        "\n",
        "$$\n",
        "\\nabla (ElasticNetLogLoss) = \\frac{1}{n} (\\hat{Y} - Y) X + \\lambda_1 \\, \\text{sgn}(W) + 2 \\lambda_2 W\n",
        "$$"
      ],
      "metadata": {
        "id": "fyoPzJ-YcYwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "class MyLogReg():\n",
        "  def __init__(self, n_iter=100, learning_rate=0.1, weights=None, metric=None, reg=None, l1_coef=0, l2_coef=0):\n",
        "    self.n_iter = n_iter\n",
        "    self.learning_rate = learning_rate\n",
        "    self.weights = weights\n",
        "    self.metric = metric\n",
        "    self.best_score = None\n",
        "    #регуляризация\n",
        "    self.reg = reg\n",
        "    self.l1_coef = l1_coef\n",
        "    self.l2_coef = l2_coef\n",
        "\n",
        "  def __str__(self):\n",
        "    params = ', '.join(f'{key}={value}' for key, value in self.__dict__.items())\n",
        "    return f'{__class__.__name__} class: {params}'\n",
        "\n",
        "  def compute_metric(self, y, y_pred):\n",
        "    y_pred_binary = (y_pred >= 0.5).astype(int)\n",
        "    # accuracy\n",
        "    if self.metric == 'accuracy':\n",
        "      return accuracy_score(y, y_pred_binary)\n",
        "    # precision\n",
        "    elif self.metric == 'precision':\n",
        "      return precision_score(y, y_pred_binary)\n",
        "    # recall\n",
        "    elif self.metric == 'recall':\n",
        "      return recall_score(y, y_pred_binary)\n",
        "    # f1\n",
        "    elif self.metric == 'f1':\n",
        "      return f1_score(y, y_pred_binary)\n",
        "    # roc_auc\n",
        "    elif self.metric == 'roc_auc':\n",
        "      # округляем предсказания до 10 знаков после запятой только для ROC-AUC\n",
        "      y_pred_rounded = np.round(y_pred, 10)\n",
        "      return roc_auc_score(y, y_pred_rounded)\n",
        "\n",
        "  def compute_log_loss(self, y, y_pred):\n",
        "    # обычный log_loss\n",
        "    log_loss = -np.mean(y * np.log(y_pred + 1e-15) + (1 - y) * np.log(1 - y_pred + 1e-15))\n",
        "    # log_loss c L1-регуляризацией (добавляем модуль суммы весов к функции потерь)\n",
        "    if self.reg == 'l1':\n",
        "      log_loss += self.l1_coef * np.sum(np.abs(self.weights))\n",
        "    # log_loss c L2-регуляризацией (добавляем сумму квадратов весов к функции потерь)\n",
        "    elif self.reg == 'l2':\n",
        "      log_loss += self.l2_coef * np.sum(self.weights ** 2)\n",
        "    #Elastic net\n",
        "    elif self.reg == 'elasticnet':\n",
        "      log_loss += self.l1_coef * np.sum(np.abs(self.weights)) + self.l2_coef * np.sum(self.weights ** 2)\n",
        "\n",
        "    return log_loss\n",
        "\n",
        "\n",
        "  def compute_gradient(self, X, y, y_pred):\n",
        "    # стоковый градиент\n",
        "    gradient = np.dot(X.T, (y_pred-y)) / y.size\n",
        "    # gradient с L1-регуляризацией\n",
        "    if self.reg == 'l1':\n",
        "      gradient += self.l1_coef * np.sign(self.weights)\n",
        "    # gradient с L2-регуляризацией\n",
        "    elif self.reg == 'l2':\n",
        "      gradient += 2 * self.l2_coef * self.weights\n",
        "    # gradient c ElasticNet\n",
        "    elif self.reg == 'elasticnet':\n",
        "      gradient += self.l1_coef * np.sign(self.weights) + 2 * self.l2_coef * self.weights\n",
        "    return gradient\n",
        "\n",
        "\n",
        "  def fit(self, X:pd.DataFrame, y:pd.Series, verbose=False):\n",
        "    # дополняем матрицу фичей единичным столбцом слева\n",
        "    # это необходимо для перемножения первоначальных значений весов\n",
        "    X = np.hstack([np.ones((X.shape[0], 1)), X.values])\n",
        "    # инициализируем веса\n",
        "    self.weights = np.ones(X.shape[1])\n",
        "    # вычисляем начальное значение функции потерь\n",
        "    for i in range(self.n_iter):\n",
        "      y_pred = 1 / (1 + np.exp(-X.dot(self.weights)))\n",
        "      # считаем logloss\n",
        "      log_loss = self.compute_log_loss(y, y_pred)\n",
        "      # вычисление градиента (частной производной функции потерь)\n",
        "      gradient = self.compute_gradient(X, y, y_pred)\n",
        "      # обновление весов (шаг в противоположную сторону от градиента)\n",
        "      self.weights -= self.learning_rate * gradient\n",
        "      # Логирование на каждой итерации с отображением метрики\n",
        "      if verbose and i % verbose == 0:\n",
        "        metric_value = self.compute_metric(y, y_pred)\n",
        "        if self.metric:\n",
        "          print(f'i | loss: {loss:.2f} | {self.metric}: {metric_value:.2f}')\n",
        "        else:\n",
        "          print( f'{i} | loss: {log_loss}')\n",
        "    # Вычисляем метрику на последнм шаге после завершения всеъ итераций\n",
        "    y_pred_final = 1 / (1 + np.exp(-X.dot(self.weights)))  # Используем логистическую функцию\n",
        "    final_metric = self.compute_metric(y, y_pred_final)\n",
        "    # Добавлена проверка на None\n",
        "    if final_metric is not None:\n",
        "        self.best_score = round(final_metric, 10)\n",
        "    else:\n",
        "        self.best_score = None\n",
        "\n",
        "\n",
        "  def get_coef(self):\n",
        "    return self.weights[1:]\n",
        "\n",
        "  def predict_proba(self, X:pd.DataFrame):\n",
        "    X = np.hstack([np.ones((X.shape[0],1)), X.values])\n",
        "    return 1 / (1 + np.exp(-X.dot(self.weights)))\n",
        "\n",
        "  def predict(self, X:pd.DataFrame):\n",
        "    X = np.hstack([np.ones((X.shape[0],1)), X.values])\n",
        "    return (1 / (1 + np.exp(-X.dot(self.weights))) > 0.5).astype(int)\n",
        "\n",
        "  def get_best_score(self):\n",
        "    return self.best_score"
      ],
      "metadata": {
        "id": "IaWvnMYLXlcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг №6 - Градиентный спуск"
      ],
      "metadata": {
        "id": "fYG6m2cr16CO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Добавим оптимальный регулятор скорости обучения.\n",
        "\n",
        "Возьмите код из предыдущего шага и модифицируйте в нем параметр learning_rate так, чтобы он принимал и число и лямбда-функцию:\n",
        "\n",
        "Если на вход пришло число, то работаем, как и раньше.\n",
        "Если на вход пришла lambda-функция, то вычисляем learning_rate на основе переданной формулы. Примерно такой:\n",
        "lambda iter: 0.5 * (0.85 ** iter)\n",
        "Можете дополнительно вывести значение learning_rate в лог тренировки.\n",
        "\n",
        "З.Ы. Напоминаю, что нумерация шагов (теперь) должна быть от 1 до n_iter (включительно)."
      ],
      "metadata": {
        "id": "UmKD8lg_aAk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "class MyLogReg():\n",
        "  def __init__(self, n_iter=100, learning_rate=0.1, weights=None, metric=None, reg=None, l1_coef=0, l2_coef=0):\n",
        "    self.n_iter = n_iter\n",
        "    self.learning_rate = learning_rate\n",
        "    self.weights = weights\n",
        "    self.metric = metric\n",
        "    self.best_score = None\n",
        "    #регуляризация\n",
        "    self.reg = reg\n",
        "    self.l1_coef = l1_coef\n",
        "    self.l2_coef = l2_coef\n",
        "\n",
        "  def __str__(self):\n",
        "    params = ', '.join(f'{key}={value}' for key, value in self.__dict__.items())\n",
        "    return f'{__class__.__name__} class: {params}'\n",
        "\n",
        "  def compute_metric(self, y, y_pred):\n",
        "    y_pred_binary = (y_pred >= 0.5).astype(int)\n",
        "    # accuracy\n",
        "    if self.metric == 'accuracy':\n",
        "      return accuracy_score(y, y_pred_binary)\n",
        "    # precision\n",
        "    elif self.metric == 'precision':\n",
        "      return precision_score(y, y_pred_binary)\n",
        "    # recall\n",
        "    elif self.metric == 'recall':\n",
        "      return recall_score(y, y_pred_binary)\n",
        "    # f1\n",
        "    elif self.metric == 'f1':\n",
        "      return f1_score(y, y_pred_binary)\n",
        "    # roc_auc\n",
        "    elif self.metric == 'roc_auc':\n",
        "      # округляем предсказания до 10 знаков после запятой только для ROC-AUC\n",
        "      y_pred_rounded = np.round(y_pred, 10)\n",
        "      return roc_auc_score(y, y_pred_rounded)\n",
        "\n",
        "  def compute_log_loss(self, y, y_pred):\n",
        "    # обычный log_loss\n",
        "    log_loss = -np.mean(y * np.log(y_pred + 1e-15) + (1 - y) * np.log(1 - y_pred + 1e-15))\n",
        "    # log_loss c L1-регуляризацией (добавляем модуль суммы весов к функции потерь)\n",
        "    if self.reg == 'l1':\n",
        "      log_loss += self.l1_coef * np.sum(np.abs(self.weights))\n",
        "    # log_loss c L2-регуляризацией (добавляем сумму квадратов весов к функции потерь)\n",
        "    elif self.reg == 'l2':\n",
        "      log_loss += self.l2_coef * np.sum(self.weights ** 2)\n",
        "    #Elastic net\n",
        "    elif self.reg == 'elasticnet':\n",
        "      log_loss += self.l1_coef * np.sum(np.abs(self.weights)) + self.l2_coef * np.sum(self.weights ** 2)\n",
        "\n",
        "    return log_loss\n",
        "\n",
        "\n",
        "  def compute_gradient(self, X, y, y_pred):\n",
        "    # стоковый градиент\n",
        "    gradient = np.dot(X.T, (y_pred-y)) / y.size\n",
        "    # gradient с L1-регуляризацией\n",
        "    if self.reg == 'l1':\n",
        "      gradient += self.l1_coef * np.sign(self.weights)\n",
        "    # gradient с L2-регуляризацией\n",
        "    elif self.reg == 'l2':\n",
        "      gradient += 2 * self.l2_coef * self.weights\n",
        "    # gradient c ElasticNet\n",
        "    elif self.reg == 'elasticnet':\n",
        "      gradient += self.l1_coef * np.sign(self.weights) + 2 * self.l2_coef * self.weights\n",
        "    return gradient\n",
        "\n",
        "\n",
        "  def fit(self, X:pd.DataFrame, y:pd.Series, verbose=False):\n",
        "    # дополняем матрицу фичей единичным столбцом слева\n",
        "    # это необходимо для перемножения первоначальных значений весов\n",
        "    X = np.hstack([np.ones((X.shape[0], 1)), X.values])\n",
        "    # инициализируем веса\n",
        "    self.weights = np.ones(X.shape[1])\n",
        "    # вычисляем начальное значение функции потерь\n",
        "    for i in range(1, self.n_iter+1):\n",
        "      y_pred = 1 / (1 + np.exp(-X.dot(self.weights)))\n",
        "      # считаем logloss\n",
        "      log_loss = self.compute_log_loss(y, y_pred)\n",
        "      # вычисление градиента (частной производной функции потерь)\n",
        "      gradient = self.compute_gradient(X, y, y_pred)\n",
        "      # обновление весов (шаг в противоположную сторону от градиента)\n",
        "      if callable(self.learning_rate):\n",
        "         lr = self.learning_rate(i)\n",
        "      else:\n",
        "         lr = self.learning_rate\n",
        "\n",
        "      self.weights -= lr * gradient\n",
        "      # Логирование на каждой итерации с отображением метрики\n",
        "      if verbose and i % verbose == 0:\n",
        "        metric_value = self.compute_metric(y, y_pred)\n",
        "        if self.metric:\n",
        "          print(f'i | loss: {loss:.2f} | {self.metric}: {metric_value:.2f}')\n",
        "        else:\n",
        "          print( f'{i} | loss: {log_loss}')\n",
        "    # Вычисляем метрику на последнм шаге после завершения всеъ итераций\n",
        "    y_pred_final = 1 / (1 + np.exp(-X.dot(self.weights)))  # Используем логистическую функцию\n",
        "    final_metric = self.compute_metric(y, y_pred_final)\n",
        "    # Добавлена проверка на None\n",
        "    if final_metric is not None:\n",
        "        self.best_score = round(final_metric, 10)\n",
        "    else:\n",
        "        self.best_score = None\n",
        "\n",
        "\n",
        "  def get_coef(self):\n",
        "    return self.weights[1:]\n",
        "\n",
        "  def predict_proba(self, X:pd.DataFrame):\n",
        "    X = np.hstack([np.ones((X.shape[0],1)), X.values])\n",
        "    return 1 / (1 + np.exp(-X.dot(self.weights)))\n",
        "\n",
        "  def predict(self, X:pd.DataFrame):\n",
        "    X = np.hstack([np.ones((X.shape[0],1)), X.values])\n",
        "    return (1 / (1 + np.exp(-X.dot(self.weights))) > 0.5).astype(int)\n",
        "\n",
        "  def get_best_score(self):\n",
        "    return self.best_score\n"
      ],
      "metadata": {
        "id": "25l84YGN1_qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг №7 - Стохастический градиентный спуск"
      ],
      "metadata": {
        "id": "tforRUQa7iBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- И последнее... научимся выполнять стохастический градиентный спуск\n",
        "\n",
        "Для этого добавьте в класс `MyLogReg` два новых параметра:\n",
        "\n",
        "- `sgd_sample` — кол-во образцов, которое будет использоваться на каждой итерации обучения. Может принимать либо целые числа, либо дробные от `0.0` до `1.0`.  \n",
        "  **По-умолчанию**: `None`\n",
        "- `random_state` — для воспроизводимости результата зафиксируем сид (об этом далее).  \n",
        "  **По-умолчанию**: `42`\n",
        "\n",
        "- Внесем изменения в алгоритм обучения:\n",
        "\n",
        "1. В начале обучения фиксируем сид (см. ниже).\n",
        "2. В начале каждого шага формируется новый мини-пакет на основе параметра `sgd_sample`:\n",
        "   - Если заданы целые числа, то из исходного датасета берется ровно столько примеров, сколько указано.\n",
        "   - Если задано дробное число, то рассматриваем его как долю от количества строк в исходном датасете (округленное до целого числа).\n",
        "3. Расчет градиента (и последующее изменение весов) делаем на основе мини-пакета.\n",
        "4. Все остальные параметры, если они заданы (например, регуляризация), также должны учитываться при обучении.\n",
        "5. Ошибку и метрику необходимо считать на всем датасете, а не на мини-пакете.\n",
        "6. Если `sgd_sample = None`, то обучение выполняется как раньше (на всех данных).\n",
        "\n",
        "- Случайная генерация\n",
        "\n",
        "Случайные подвыборки будем генерировать, как и раньше.\n",
        "\n",
        "- В начале обучения посредством модуля `random` фиксируем сид:\n",
        "\n",
        "    ```python\n",
        "    random.seed(<random_state>)\n",
        "    ```\n",
        "\n",
        "- В начале каждой итерации сформируем номера строк, которые стоит отобрать:\n",
        "\n",
        "    ```python\n",
        "    sample_rows_idx = random.sample(range(X.shape[0]), <sgd_sample>)\n",
        "    ```\n",
        "\n",
        "**З.Ы.** Модуль `random` уже импортирован.\n"
      ],
      "metadata": {
        "id": "uE6bGHOv7pfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "class MyLogReg():\n",
        "  def __init__(self, n_iter=100, learning_rate=0.1, weights=None, metric=None, reg=None, l1_coef=0, l2_coef=0, sgd_sample=None, random_state=42):\n",
        "    self.n_iter = n_iter\n",
        "    self.learning_rate = learning_rate\n",
        "    self.weights = weights\n",
        "    self.metric = metric\n",
        "    self.best_score = None\n",
        "    #регуляризация\n",
        "    self.reg = reg\n",
        "    self.l1_coef = l1_coef\n",
        "    self.l2_coef = l2_coef\n",
        "    # stochastic_grad\n",
        "    self.sgd_sample = sgd_sample\n",
        "    self.random_state = random_state\n",
        "\n",
        "  def __str__(self):\n",
        "    params = ', '.join(f'{key}={value}' for key, value in self.__dict__.items())\n",
        "    return f'{__class__.__name__} class: {params}'\n",
        "\n",
        "  def compute_metric(self, y, y_pred):\n",
        "    y_pred_binary = (y_pred >= 0.5).astype(int)\n",
        "    # accuracy\n",
        "    if self.metric == 'accuracy':\n",
        "      return accuracy_score(y, y_pred_binary)\n",
        "    # precision\n",
        "    elif self.metric == 'precision':\n",
        "      return precision_score(y, y_pred_binary)\n",
        "    # recall\n",
        "    elif self.metric == 'recall':\n",
        "      return recall_score(y, y_pred_binary)\n",
        "    # f1\n",
        "    elif self.metric == 'f1':\n",
        "      return f1_score(y, y_pred_binary)\n",
        "    # roc_auc\n",
        "    elif self.metric == 'roc_auc':\n",
        "      # округляем предсказания до 10 знаков после запятой только для ROC-AUC\n",
        "      y_pred_rounded = np.round(y_pred, 10)\n",
        "      return roc_auc_score(y, y_pred_rounded)\n",
        "\n",
        "  def compute_log_loss(self, y, y_pred):\n",
        "    # обычный log_loss\n",
        "    log_loss = -np.mean(y * np.log(y_pred + 1e-15) + (1 - y) * np.log(1 - y_pred + 1e-15))\n",
        "    # log_loss c L1-регуляризацией (добавляем модуль суммы весов к функции потерь)\n",
        "    if self.reg == 'l1':\n",
        "      log_loss += self.l1_coef * np.sum(np.abs(self.weights))\n",
        "    # log_loss c L2-регуляризацией (добавляем сумму квадратов весов к функции потерь)\n",
        "    elif self.reg == 'l2':\n",
        "      log_loss += self.l2_coef * np.sum(self.weights ** 2)\n",
        "    #Elastic net\n",
        "    elif self.reg == 'elasticnet':\n",
        "      log_loss += self.l1_coef * np.sum(np.abs(self.weights)) + self.l2_coef * np.sum(self.weights ** 2)\n",
        "\n",
        "    return log_loss\n",
        "\n",
        "\n",
        "  def compute_gradient(self, X, y, y_pred):\n",
        "    # стоковый градиент\n",
        "    gradient = np.dot(X.T, (y_pred-y)) / y.size\n",
        "    # gradient с L1-регуляризацией\n",
        "    if self.reg == 'l1':\n",
        "      gradient += self.l1_coef * np.sign(self.weights)\n",
        "    # gradient с L2-регуляризацией\n",
        "    elif self.reg == 'l2':\n",
        "      gradient += 2 * self.l2_coef * self.weights\n",
        "    # gradient c ElasticNet\n",
        "    elif self.reg == 'elasticnet':\n",
        "      gradient += self.l1_coef * np.sign(self.weights) + 2 * self.l2_coef * self.weights\n",
        "    return gradient\n",
        "\n",
        "\n",
        "  def fit(self, X:pd.DataFrame, y:pd.Series, verbose=False):\n",
        "    # фиксируем сид для воспроизводимости\n",
        "    random.seed(self.random_state)\n",
        "    # дополняем матрицу фичей единичным столбцом слева\n",
        "    # это необходимо для перемножения первоначальных значений весов\n",
        "    X = np.hstack([np.ones((X.shape[0], 1)), X.values])\n",
        "    # инициализируем веса\n",
        "    self.weights = np.ones(X.shape[1])\n",
        "    # Определяем кол-во примеров для мини-батча\n",
        "    if self.sgd_sample is not None:\n",
        "      if isinstance(self.sgd_sample, float) and 0 < self.sgd_sample <= 1:\n",
        "        batch_size = int(self.sgd_sample * X.shape[0]) # Интерпретируем как долю от данных\n",
        "      elif isinstance(self.sgd_sample, int) and self.sgd_sample > 0:\n",
        "        batch_size = self.sgd_sample # Определенное кол-во строк\n",
        "      else:\n",
        "        raise ValueError('sgd_sample должен быть целым числом или дробным значением от 0 до 1.')\n",
        "    else:\n",
        "      batch_size = X.shape[0] # Если не задано, то берем все данные\n",
        "    # вычисляем начальное значение функции потерь\n",
        "    for i in range(1, self.n_iter+1):\n",
        "      # формируем мини-батч\n",
        "      if batch_size < X.shape[0]:\n",
        "        sample_rows_idx = random.sample(range(X.shape[0]), batch_size)\n",
        "        X_batch = X[sample_rows_idx]\n",
        "        y_batch = y.values[sample_rows_idx]\n",
        "      else:\n",
        "        X_batch = X\n",
        "        y_batch = y.values\n",
        "      # Предсказания для мини-батча\n",
        "      y_pred = 1 / (1 + np.exp(-X_batch.dot(self.weights)))\n",
        "      # считаем logloss\n",
        "      log_loss = self.compute_log_loss(y_batch, y_pred)\n",
        "      # вычисление градиента (частной производной функции потерь)\n",
        "      gradient = self.compute_gradient(X_batch, y_batch, y_pred)\n",
        "      # обновление весов (шаг в противоположную сторону от градиента)\n",
        "      if callable(self.learning_rate):\n",
        "         lr = self.learning_rate(i)\n",
        "      else:\n",
        "         lr = self.learning_rate\n",
        "\n",
        "      self.weights -= lr * gradient\n",
        "      # Логирование на каждой итерации с отображением метрики\n",
        "      if verbose and i % verbose == 0:\n",
        "        metric_value = self.compute_metric(y, y_pred)\n",
        "        if self.metric:\n",
        "          print(f'i | loss: {loss:.2f} | {self.metric}: {metric_value:.2f}')\n",
        "        else:\n",
        "          print( f'{i} | loss: {log_loss}')\n",
        "    # Вычисляем метрику на последнм шаге после завершения всеъ итераций\n",
        "    y_pred_final = 1 / (1 + np.exp(-X.dot(self.weights)))  # Используем логистическую функцию\n",
        "    final_metric = self.compute_metric(y, y_pred_final)\n",
        "    # Добавлена проверка на None\n",
        "    if final_metric is not None:\n",
        "        self.best_score = round(final_metric, 10)\n",
        "    else:\n",
        "        self.best_score = None\n",
        "\n",
        "\n",
        "  def get_coef(self):\n",
        "    return self.weights[1:]\n",
        "\n",
        "  def predict_proba(self, X:pd.DataFrame):\n",
        "    X = np.hstack([np.ones((X.shape[0],1)), X.values])\n",
        "    return 1 / (1 + np.exp(-X.dot(self.weights)))\n",
        "\n",
        "  def predict(self, X:pd.DataFrame):\n",
        "    X = np.hstack([np.ones((X.shape[0],1)), X.values])\n",
        "    return (1 / (1 + np.exp(-X.dot(self.weights))) > 0.5).astype(int)\n",
        "\n",
        "  def get_best_score(self):\n",
        "    return self.best_score"
      ],
      "metadata": {
        "id": "rysH-X658b54"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}