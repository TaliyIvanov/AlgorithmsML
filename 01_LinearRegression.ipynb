{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "X3gc91Z7x7b1",
        "SpKqBBAfx7b2",
        "c8wuxH4Tx80t",
        "sjg9ny2KxzLU",
        "TrKhx0C8x5_p",
        "aYExSS1g6iwa"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг №1 - Инициализация класса\n"
      ],
      "metadata": {
        "id": "X3gc91Z7x7b1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Инициализация класса\n",
        "Приступим к реализации. И первое, что мы сделаем — это создадим класс MyLineReg.\n",
        "\n",
        "Данный класс при инициализации должен принимать на вход два параметра:\n",
        "\n",
        "n_iter — количество шагов градиентного спуска.\n",
        "По-умолчанию: 100\n",
        "\n",
        "learning_rate — коэффициент скорости обучения градиентного спуска.\n",
        "По-умолчанию: 0.1\n",
        "\n",
        "Все переданные (или дефолтные) параметры должны быть сохранены внутри класса.\n",
        "\n",
        "При обращении к экземпляру класса (или при передачи его в функцию print) необходимо распечатать строку по следующему шаблону (строго в таком виде):\n",
        "\n",
        "MyLineReg class: n_iter=<n_iter>, learning_rate=<learning_rate>\n",
        "\n",
        "Примечания:\n",
        "\n",
        "Параметры должны распечатываться точно в таком же порядке, как приведены выше.\n",
        "В следующих уроках будут добавляться новые параметры. Если вы захардкодите вывод параметров, то в последующем их придется вручную добавлять в распечатку. Либо можете сделать динамический вывод всех имеющихся параметров.\n",
        "(Здесь и далее) вам необходимо написать только класс. Создавать экземпляр класса не нужно."
      ],
      "metadata": {
        "id": "I9_1eK6o6es4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyLineReg():\n",
        "  def __init__(self, n_iter=100, learning_rate=0.1):\n",
        "    self.n_iter = n_iter\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "  def __str__(self):\n",
        "    params = \", \".join(f\"{key}={value}\" for key, value in self.__dict__.items())\n",
        "    return f\"{__class__.__name__} class: {params}\""
      ],
      "metadata": {
        "id": "yCV5lSSfyCKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг №2 - Метод fit.\n"
      ],
      "metadata": {
        "id": "SpKqBBAfx7b2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Пора научить нашу модель чему-то полезному :) Что для это нужно сделать:\n",
        "\n",
        "1. В инициализатор класса добавить новый параметр — weights — который будет хранить веса модели. По умолчанию он ничего не содержит.\n",
        "2. Вам необходимо реализовать метод fit в Вашем классе. Данный метод должен делать следующее:\n",
        "  1. На вход принимать три атрибута:\n",
        "  - X — все фичи в виде датафрейма пандаса.\n",
        "  Примечание: даже если фича будет всего одна это все равно будет датафрейм, а не серия.\n",
        "  - y — целевая переменная в виде пандасовской серии.\n",
        "  - verbose — указывает на какой итерации выводить лог. Например,значение 10 означает, что на каждой 10 итерации градиентного спуска будет печататься лог. Значение по умолчанию: False (т.е. ничего не выводится).\n",
        "  2. Дополнить переданную матрицу фичей единичным столбцом слева.\n",
        "  3. Определить сколько фичей передано и создать вектор весов, состоящий из одних единиц соответствующей длинны: т.е. количество фичей + 1.\n",
        "  4. Дальше в цикле (до n_iter):\n",
        "  - Предсказать целевую переменную\n",
        "  - Посчитать ошибку (MSE)\n",
        "  - Вычислить градиент\n",
        "  - Сделать шаг размером learning rate в противоположную от градиента сторону\n",
        "  - Сохранить обновленные веса внутри класса\n",
        "  5. В процессе обучения необходимо выводить лог, в котором указывать номер итерации и значение функций потерь:\n",
        "  start | loss: 42027.65\n",
        "  100 | loss: 1222.87\n",
        "  200 | loss: 232.17\n",
        "  300 | loss: 202.4\n",
        "  где start - значении функции потерь до начала обучения. Далее выводится каждое i-ое значение итерации переданное в параметре verbose. Если verbose = False, то лог не выводится вовсе.\n",
        "  З.Ы. Данный вывод никак проверяться не будет. Он в основном нужен для отладки. Поэтому можете модифицировать его внешний вид под свои нужды.\n",
        "  6. Метод ничего не возвращает.\n",
        "\n",
        "3. Необходимо реализовать метод get_coef, который будет возвращать значения весов в виде вектора NumPy, начиная со второго значения. Первое значение нам не нужно, потому что оно соответствует фиктивной фиче (единичке). Все же остальные могут использоваться для оценки важности фичей."
      ],
      "metadata": {
        "id": "1glsF4bl6bYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "sunDh0-UEwYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class MyLineReg:\n",
        "    def __init__(self, n_iter=100, learning_rate=0.1, weights=None):\n",
        "        self.n_iter = n_iter\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = weights\n",
        "\n",
        "    def __str__(self):\n",
        "        params = ', '.join(f\"{key}={value}\" for key, value in self.__dict__.items() if key != 'weights')\n",
        "        return f\"{__class__.__name__} class: {params}\"\n",
        "\n",
        "    __repr__ = __str__\n",
        "\n",
        "    def fit(self, X: pd.DataFrame, y: pd.Series, verbose=False):\n",
        "        # Добавляем столбец единичек для фиктивной фичи (свободного члена)\n",
        "        X = np.hstack([np.ones((X.shape[0], 1)), X.values])\n",
        "\n",
        "        # Инициализируем веса\n",
        "        self.weights = np.ones(X.shape[1])\n",
        "\n",
        "        # Вычисляем начальное значение функции потерь\n",
        "        y_pred = X.dot(self.weights)\n",
        "        loss = np.mean((y_pred - y) ** 2)\n",
        "\n",
        "        # Лог перед началом обучения\n",
        "        if verbose:\n",
        "            print(f\"start | loss: {loss:.2f}\")\n",
        "\n",
        "        # Градиентный спуск\n",
        "        for i in range(1, self.n_iter + 1):\n",
        "            # Предсказания\n",
        "            y_pred = X.dot(self.weights)\n",
        "\n",
        "            # Ошибка (Mean Squared Error)\n",
        "            loss = np.mean((y_pred - y) ** 2)\n",
        "\n",
        "            # Градиент\n",
        "            gradient = (2 / X.shape[0]) * X.T.dot(y_pred - y)\n",
        "\n",
        "            # Обновление весов\n",
        "            self.weights -= self.learning_rate * gradient\n",
        "\n",
        "            # Логирование каждые 'verbose' итераций\n",
        "            if verbose and i % verbose == 0:\n",
        "                print(f\"{i} | loss: {loss:.2f}\")\n",
        "\n",
        "    def get_coef(self):\n",
        "        # Возвращаем все веса, начиная с первого (пропускаем фиктивный признак)\n",
        "        return self.weights[1:]"
      ],
      "metadata": {
        "id": "Rm_fVBX2x7b2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# создадим набор тестовых данных для проверки класса\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Задание параметров\n",
        "np.random.seed(42)  # Для воспроизводимости\n",
        "\n",
        "# Количество примеров\n",
        "n_samples = 1000\n",
        "\n",
        "# Генерация данных признаков X (два признака x1 и x2)\n",
        "X = np.random.rand(n_samples, 2) * 10  # Признаки от 0 до 10\n",
        "\n",
        "# Коэффициенты, которые мы знаем (истинные веса)\n",
        "true_weights = np.array([2, 7])  # для x1 и x2\n",
        "bias = 7  # Свободный член (константа)\n",
        "\n",
        "# Генерация целевой переменной y с шумом\n",
        "y = X.dot(true_weights) + bias + np.random.randn(n_samples) * 0.5  # Немного шума\n",
        "\n",
        "# Превращаем в DataFrame для удобства работы\n",
        "X_df = pd.DataFrame(X, columns=['x1', 'x2'])\n",
        "y_series = pd.Series(y)\n",
        "\n",
        "# Выводим примеры данных\n",
        "print('Признаки')\n",
        "print(X_df.head(10))  # Признаки\n",
        "print('Целевая переменная')\n",
        "print(y_series.head(10))  # Целевая переменная"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3wH8YkEPnyv",
        "outputId": "82a043eb-5dea-44d0-c646-ddefa58078f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Признаки\n",
            "         x1        x2\n",
            "0  3.745401  9.507143\n",
            "1  7.319939  5.986585\n",
            "2  1.560186  1.559945\n",
            "3  0.580836  8.661761\n",
            "4  6.011150  7.080726\n",
            "5  0.205845  9.699099\n",
            "6  8.324426  2.123391\n",
            "7  1.818250  1.834045\n",
            "8  3.042422  5.247564\n",
            "9  4.319450  2.912291\n",
            "Целевая переменная\n",
            "0    80.601813\n",
            "1    63.132533\n",
            "2    20.926750\n",
            "3    68.977685\n",
            "4    69.044173\n",
            "5    74.903790\n",
            "6    39.258935\n",
            "7    23.339253\n",
            "8    49.807111\n",
            "9    35.651334\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем объект класса MyLineReg\n",
        "model = MyLineReg(n_iter=1000, learning_rate=0.01)\n",
        "\n",
        "# Обучаем модель\n",
        "model.fit(X_df, y_series, verbose=100)\n",
        "\n",
        "# Получаем коэффициенты\n",
        "print(\"Оцененные коэффициенты:\", model.get_coef())\n",
        "\n",
        "# Истинные коэффициенты (должны быть близки к 2 и 7)\n",
        "print(\"Истинные коэффициенты: [2, 7]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwmAYhr2Pzi2",
        "outputId": "d0065ed9-7195-4df7-9cf9-9f4e5f9564e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start | loss: 2015.46\n",
            "100 | loss: 2.60\n",
            "200 | loss: 1.57\n",
            "300 | loss: 0.98\n",
            "400 | loss: 0.66\n",
            "500 | loss: 0.47\n",
            "600 | loss: 0.37\n",
            "700 | loss: 0.31\n",
            "800 | loss: 0.28\n",
            "900 | loss: 0.26\n",
            "1000 | loss: 0.25\n",
            "Оцененные коэффициенты: [2.02176664 7.01914741]\n",
            "Истинные коэффициенты: [2, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг 3: Добавление метода `predict` для предсказаний\n"
      ],
      "metadata": {
        "id": "c8wuxH4Tx80t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Теперь добавим третий шаг, чтобы наша модель могла выдавать предсказания.\n",
        "\n",
        "#### Задача:\n",
        "\n",
        "Добавьте в класс `MyLineReg` метод `predict`. Этот метод должен выполнять следующие действия:\n",
        "\n",
        "1. На вход принимать матрицу фичей в виде датафрейма Pandas.\n",
        "2. Дополнять матрицу фичей единичным вектором (первый столбец).\n",
        "3. Возвращать вектор предсказаний.\n",
        "\n",
        "#### Напоминание:\n",
        "\n",
        "Предсказание вычисляется следующим образом:\n",
        "\n",
        "$$\n",
        "\\hat{y} = XW\n",
        "$$\n",
        "\n",
        "Где:\n",
        "- $X$ — матрица фичей (с единичным столбцом для свободного члена).\n",
        "- $W$ — вектор весов, обученный моделью."
      ],
      "metadata": {
        "id": "KvVc7FdE6S1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Zati4ka():\n",
        "  # добавляем в наш класс метод предикт\n",
        "  def predict(self, X: pd.DataFrame):\n",
        "    # дополняем матрицу фичей единичным вектором\n",
        "    X = np.hstack([np.ones((X.shape[0], 1)), X.values])\n",
        "    # возвращаем предсказанные значения\n",
        "    return X.dot(self.weights)"
      ],
      "metadata": {
        "id": "XSb_2Ofpx80t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Теперь наш класс будет выглядеть так:\n",
        "class MyLineReg():\n",
        "  # инициализация класса\n",
        "  def __init__(self, n_iter=100, learning_rate=0.1, weights=None):\n",
        "    self.n_iter = n_iter\n",
        "    self.learning_rate = learning_rate\n",
        "    self.weights = weights\n",
        "\n",
        "  # метод str, для вывода информации\n",
        "  def __str__(self):\n",
        "    params = ', '.join(f\"{key}={value}\" for key, value in self.__dict__.items() if key != 'weights')\n",
        "    return f\"{__class__.__name__} class: {params}\"\n",
        "\n",
        "  # реализация метода fit\n",
        "  # метод принимает:\n",
        "  # X - все фичи в виде датафрейма пандаса;\n",
        "  # y — целевая переменная в виде пандасовской серии\n",
        "  # verbose — указывает на какой итерации выводить лог\n",
        "  def fit(self, X: pd.DataFrame, y:pd.Series, verbose=False):\n",
        "\n",
        "    # Дополним переданную матрицу фичей - единичным столбцом слева\n",
        "    X = np.hstack([np.ones((X.shape[0], 1)), X.values])\n",
        "\n",
        "    # Инициализация весов\n",
        "    self.weights = np.ones(X.shape[1])\n",
        "\n",
        "    # Вычисляем начальное значение функции потерь\n",
        "    # .dot - выполняет метричное умножение X * self.weights\n",
        "    y_pred = X.dot(self.weights)\n",
        "    loss = np.mean((y_pred - y) ** 2)\n",
        "\n",
        "    # Лог перед началом обучения\n",
        "    if verbose:\n",
        "      print(f\"start | loss: {loss:.2f}\")\n",
        "\n",
        "    # реализация градиентного спуска\n",
        "    for i in range(1, self.n_iter + 1):\n",
        "      # Предсказания\n",
        "      y_pred = X.dot(self.weights)\n",
        "\n",
        "      # Функция потерь (Mean Squared Error)\n",
        "      loss = np.mean((y - y_pred) ** 2)\n",
        "\n",
        "      # вычисляем градиент функции потерь\n",
        "      gradient = (2 / X.shape[0]) * X.T.dot(y_pred - y)\n",
        "\n",
        "      # Обновление весов\n",
        "      self.weights -= self.learning_rate * gradient\n",
        "\n",
        "      # Логирование каждые 'verbose' итераций\n",
        "      if verbose and i % verbose == 0:\n",
        "        print(f\"{i} | loss: {loss:.2f}\")\n",
        "\n",
        "  def get_coef(self):\n",
        "    #возвращаем все веса, начиная с первого (пропускаем фиктивный признак)\n",
        "\n",
        "    return self.weights[1:]\n",
        "\n",
        "    # добавляем в наш класс метод предикт\n",
        "  def predict(self, X: pd.DataFrame):\n",
        "    # дополняем матрицу фичей единичным вектором\n",
        "    X = np.hstack([np.ones((X.shape[0], 1)), X.values])\n",
        "    # возвращаем предсказанные значения\n",
        "    return X.dot(self.weights)"
      ],
      "metadata": {
        "id": "APJmebFRUfOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Опробуем метод предикт на новых данных"
      ],
      "metadata": {
        "id": "vacNgZQkVB34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем объект класса MyLineReg\n",
        "model = MyLineReg(n_iter=1000, learning_rate=0.01)\n",
        "\n",
        "# Обучаем модель\n",
        "model.fit(X_df, y_series, verbose=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmAXLAGEVNEc",
        "outputId": "3668e82b-e414-4baa-99bd-afcae3871258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start | loss: 2015.46\n",
            "100 | loss: 2.60\n",
            "200 | loss: 1.57\n",
            "300 | loss: 0.98\n",
            "400 | loss: 0.66\n",
            "500 | loss: 0.47\n",
            "600 | loss: 0.37\n",
            "700 | loss: 0.31\n",
            "800 | loss: 0.28\n",
            "900 | loss: 0.26\n",
            "1000 | loss: 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# создадим набор новых тестовых данных для проверки класса\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Задание параметров\n",
        "np.random.seed(42)  # Для воспроизводимости\n",
        "\n",
        "# Количество примеров\n",
        "n_samples = 1000\n",
        "\n",
        "# Генерация данных признаков X (два признака x1 и x2)\n",
        "Test_data = np.random.rand(n_samples, 2) * 10  # Признаки от 0 до 10\n",
        "\n",
        "# Коэффициенты, которые мы знаем (истинные веса)\n",
        "true_weights = np.array([14, 72])  # для x1 и x2\n",
        "bias = 7  # Свободный член (константа)\n",
        "\n",
        "# Генерация целевой переменной y с шумом\n",
        "y_test = Test_data.dot(true_weights) + bias + np.random.randn(n_samples) * 0.5  # Немного шума\n",
        "\n",
        "# Превращаем в DataFrame для удобства работы\n",
        "Test_df = pd.DataFrame(Test_data, columns=['x1', 'x2'])\n",
        "y_series_test = pd.Series(y_test)\n",
        "\n",
        "# Выводим примеры данных\n",
        "print('Признаки')\n",
        "print(X_df.head(10))  # Признаки\n",
        "print('Целевая переменная')\n",
        "print(y_series.head(10))  # Целевая переменная"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FOxmVbKVQOZ",
        "outputId": "3a529a49-536a-4df1-b498-9838b757c81e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Признаки\n",
            "         x1        x2\n",
            "0  3.745401  9.507143\n",
            "1  7.319939  5.986585\n",
            "2  1.560186  1.559945\n",
            "3  0.580836  8.661761\n",
            "4  6.011150  7.080726\n",
            "5  0.205845  9.699099\n",
            "6  8.324426  2.123391\n",
            "7  1.818250  1.834045\n",
            "8  3.042422  5.247564\n",
            "9  4.319450  2.912291\n",
            "Целевая переменная\n",
            "0    80.601813\n",
            "1    63.132533\n",
            "2    20.926750\n",
            "3    68.977685\n",
            "4    69.044173\n",
            "5    74.903790\n",
            "6    39.258935\n",
            "7    23.339253\n",
            "8    49.807111\n",
            "9    35.651334\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(Test_df)"
      ],
      "metadata": {
        "id": "K_lqx4NSVayh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Предсказания:\", y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq9d01gMV3Qk",
        "outputId": "91c09ccb-cf53-4617-eefe-070369f8700d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предсказания: [81.06413081 63.57969585 20.8635832  68.73226061 68.61356581 75.25533772\n",
            " 38.49420785 23.30927448 49.74426068 35.93448798 28.92128247 38.38172592\n",
            " 71.09309398 46.89157621 21.99742812 31.0122438  74.67863103 83.02519287\n",
            " 19.77409428 51.48831248 43.98429061 71.28156011 58.49510813 49.56617662\n",
            " 30.78817782 80.77021908 88.563492   83.55561883 22.30520294 30.50957076\n",
            " 33.66430136 48.55594543 50.53224329 65.91631258 77.5380539  36.32089879\n",
            " 64.10984907 72.22085876 27.55035489 22.14012565 67.9599165  17.91100523\n",
            " 35.8721986  66.26179809 57.84259799 59.2413846  61.53793723 57.0071631\n",
            " 47.33793732 14.84673877 52.06577471 48.81263589 42.6068303  68.08999823\n",
            " 16.78886338 23.93422307 82.27924271 80.73481432 36.10376646 62.66245212\n",
            " 85.98228999 20.91376003 41.34740739 83.71406312 42.75042694 30.78891609\n",
            " 32.88086961 48.50928912 66.5944378  82.3223239  43.89117389 37.93206154\n",
            " 15.10772435 54.36751503 27.35915133 41.93798398 44.04461196 43.67753344\n",
            " 73.80798081 62.67882058 58.57796005 57.17507224 67.21632695 26.33719858\n",
            " 49.05978995 21.62285946 33.011164   32.04268998 47.87443778 35.35105727\n",
            " 21.62020448 87.03665463 58.29999588 62.25245425 34.44403256 71.61898235\n",
            " 69.40244148 38.12569107 84.40638634 79.4351966  25.64605487 73.09836349\n",
            " 19.66590009 55.38345007 18.14945263 66.41902557 35.68269373 37.81123014\n",
            " 65.7359197  79.50206928 59.94558173 34.46416451 29.24753077 54.02387661\n",
            " 69.09541522 58.10983333 52.99395827 61.41710155 14.14310054 32.24136827\n",
            " 92.73129595 51.23817291 72.23231007 83.26757598 86.11590337 39.74341175\n",
            " 46.21298505 49.26921423 74.54198741 25.10603296 88.68711514 45.97425929\n",
            " 76.49384236 70.16019015 34.63548154 79.98621223 88.3916445  52.30008308\n",
            " 68.52135707 76.80975772 48.47797444 20.94990679 20.97408121 54.26209726\n",
            " 54.02441798  9.99793335 48.67314084 45.98565635 37.4760339  25.34382128\n",
            " 45.10121368 62.43228999 89.93613334 39.86694415 41.84668921 21.14171185\n",
            " 74.84200915 72.51278769 27.19162303 27.48747489 68.02238771 39.75638825\n",
            " 77.85898505 60.90509125 32.63170246 67.15103752 15.19807753 10.54864579\n",
            " 73.44596415 23.2135773  49.93277233 40.71418946 58.04402702 22.77983185\n",
            " 58.26358797 77.05029056 31.51362077 53.27885654 48.4122134  66.16240725\n",
            " 59.76100343 54.32221584 52.89509978 89.74069133 15.58677685 10.07627786\n",
            " 56.61043354 30.58840307 25.47466372 43.01021925 58.05379078 81.0665758\n",
            " 78.01869709 24.91658909 78.56836338 55.74771056 68.7786728  78.98124602\n",
            " 54.22659086 74.90310239 72.19620797 74.98505677 76.08851658 15.38922549\n",
            " 31.47202223 79.90179157 66.22927389 51.01109126 35.75725153 73.16679612\n",
            " 78.18810748 43.30782961 46.49571115 77.99574656 22.07155416 63.10227693\n",
            " 26.35664132 57.66226501 65.91843227 26.74832563 77.72867203 40.3425658\n",
            " 89.68144951 79.57513099 20.22789422 61.66707956 78.95489443 43.58600088\n",
            " 39.88529055 16.23809058 54.70489958 62.79056509 52.52536723 73.50444988\n",
            " 78.89036102 75.08272467 70.1830065  87.12631277 47.12760801 63.07380913\n",
            " 68.32036281 59.99875137 81.82684601 27.39980524 88.46379469 59.50010243\n",
            " 25.57226678 40.93697846 24.01623837 95.68334012 58.50435988 70.13916316\n",
            " 32.01775416 82.91168855 76.90182491 48.51868409 86.40765942  9.52471784\n",
            " 71.26490891 37.2781678  45.50699234 85.47873935 56.60707892 34.33729277\n",
            " 68.59568428 93.36638361 65.89600632 88.62139531 41.29938043 24.93460582\n",
            " 68.59674159 58.53005018 44.39684475 56.1943471  71.48933894 77.09213287\n",
            " 57.29109649 52.80471229 11.48953871 65.56731997 35.94288335 10.53778957\n",
            " 55.25501938 45.39703395 49.4843085  72.1571932  58.44211722 90.84299907\n",
            " 74.77179888 34.83576238 84.83234969 39.79491347 36.4218272  17.33980217\n",
            " 20.01082708 54.55046954 34.69985207 58.15880014 32.35161049 13.5160287\n",
            " 29.72990435 16.56466557 41.54148179 36.49998984 65.3975182  63.66635906\n",
            " 25.19323264 89.05887747 27.42906912 75.58072424 25.18488706 48.25922899\n",
            " 45.15402594 68.57722461 25.22033504 84.01791654 54.45451878 40.33106656\n",
            " 34.54740534 38.68536642 29.77276922 34.02227222 71.6884984  66.42801208\n",
            " 57.70135857 46.21739033 70.94781853 24.44196327 37.98233324 59.88247493\n",
            " 83.20416936 35.65841647 19.54697743 23.0109188  30.54296504 73.2102616\n",
            " 45.19813175 84.0116344  36.95100116 87.11141585 41.38163521 57.14776651\n",
            " 64.62958694 37.96858501 35.44599291 43.16581462 34.0375588  51.94181853\n",
            " 53.39316883 43.65255191 21.16505097 22.99988237 77.52782703 70.12328871\n",
            " 59.74368551 63.93855953 45.20165596 79.0580881  91.28961252 60.57343028\n",
            " 38.45183545 60.12451523 77.46347356 65.40662122 22.32345081 67.2570365\n",
            " 56.92323375 75.58647908 72.01240769 42.09633328 37.26822785 57.16136733\n",
            " 74.59702918 52.67392625 68.9027009  75.09137277 42.95807584 65.30931983\n",
            " 56.74060958 80.1849878  31.70650861 64.00232248 47.43910966 83.71637286\n",
            " 71.32112053 68.41021468 37.22346884 33.402142   62.0906657  66.32032099\n",
            " 58.8844119  36.11546579 74.19673112 46.69093757 82.59165823 51.00850861\n",
            " 59.90652771 68.34157344 65.27255274 85.5960104  31.76542011 60.99719431\n",
            " 67.02680394 90.61750393 23.82303042 85.55444916 73.91540267 60.76504754\n",
            " 46.72056251 31.39490609 23.31671678 41.45038758 82.19067172 38.1152295\n",
            " 53.41216313 43.05814699 21.98842789 46.24059325 50.69277627 62.877469\n",
            " 53.59917064 77.8181791  38.86845691 78.45740936 58.70392824 60.7567258\n",
            " 74.03932067 24.29186207 40.63378877 22.79172092 54.25613519 48.2994995\n",
            " 54.14423872 22.66816421 90.59229818 33.31117598 48.46958695 18.2335989\n",
            " 67.48589551 22.88813603 58.59270029 14.46580096 35.46451464 36.67516861\n",
            " 25.83885249 81.42491339 51.42377074 34.65503733 75.60166042 13.02460342\n",
            " 12.82507451 51.35251275 77.93792928 47.27923007 38.6558713  37.50993337\n",
            " 48.11580683 48.53323475 79.28651124 32.03284261 58.53167519 80.98854301\n",
            " 16.93374371 50.2098982  37.66873316 68.18220373 63.56104875 29.34437223\n",
            " 71.30756258 49.6069584  51.19909297 78.17076545 11.70493488 72.70090821\n",
            " 66.36108939 65.5807191  74.48928544 42.11938589 20.54768055 61.92529417\n",
            " 50.76989654 39.0453845  37.56167097 42.15260649 51.62582458 85.93510763\n",
            " 36.65764534 25.79891513 22.4005058  58.6884116  67.62886049 14.18896463\n",
            " 56.26130828 51.19882692 69.23543824 71.68385793 53.69106519 20.48609561\n",
            " 34.9092063  57.27714953 48.53954714 75.80463688 69.30752488 80.36227625\n",
            " 46.16092921 80.55797711 28.61386786 85.96939179 50.27549006 27.65152033\n",
            " 41.7313405  93.60890859 59.13642482 83.468423   71.39403097 49.36103477\n",
            " 19.87277443 57.18911476 64.50369173 30.21181131 73.16904455 57.13643514\n",
            " 67.90051261 13.18238882 25.84406513 61.5710688  81.81329529 51.5526433\n",
            " 50.48264525 60.50726371 93.4977468  51.6161269  67.05060935 19.34162257\n",
            " 34.09112577 29.14769618  8.86538894 56.75523366 62.23539761 77.85152515\n",
            " 28.93371629 76.90888376 58.66764298 30.88587079 43.06025991 71.41925749\n",
            " 55.03314716 23.81413313 24.30385811 47.09695193 75.95195111 89.74815358\n",
            " 39.0804434  67.99273031 31.63385824 39.0078216  48.9758612  59.46473287\n",
            " 70.19968679 21.22042135 91.80676461 81.01110018 58.67271968 37.18836745\n",
            " 32.52715814 56.19188038 23.85331391 25.91740006 58.7242471  58.0580798\n",
            " 47.84613071 60.17722992 73.83078743 69.09956322 84.83320112 79.83241074\n",
            " 68.60109373 72.10093402 71.7960072  26.43931657 32.50141859 65.72302674\n",
            " 67.42426616 59.20348454 27.03650215 88.10349185 37.7695565  57.48025063\n",
            " 30.03554784 60.13827233 50.22572742 84.97033004 74.61772158 25.42879229\n",
            " 62.55218884 48.9848398  57.33568877 68.85901535 94.44840514 31.04692066\n",
            " 23.81586931 29.92172994 70.71502235 51.87297128 82.2814962  47.05967706\n",
            " 65.85671071 36.28054544 56.19056666 39.17294345 36.58698345 81.73435822\n",
            " 60.06474942 34.04576522 55.38525418 40.97721725 13.25359204 45.80235483\n",
            " 57.4503727  39.35668622 78.83177635 81.64258578 66.10122169 11.69629069\n",
            " 68.66075421 56.66265393 17.91843817 50.82410039 45.31965296 48.90735048\n",
            " 68.18003289 44.98713289 12.07985909 48.38268311 39.36300666 81.18393592\n",
            " 25.30743595 23.83969811 80.90865226 44.65838728 68.56448324 18.76038562\n",
            " 49.69899426 33.14724176 12.94659497 24.10014185 66.62867909 36.99460055\n",
            " 17.48564288 69.21547216 73.11248078 45.8048908  82.22992076 77.47141238\n",
            " 70.81643164 71.18800039 75.19814026 61.07825234 68.75374539 12.64633453\n",
            " 22.95429208 39.77214729 50.13669549 65.8718666  55.04639083 68.46975303\n",
            " 81.31273302 72.11512657 58.54027079 10.37099271 44.441699   75.35527782\n",
            " 51.64156209 39.79752104 29.54305773 64.90719854 66.53444954 77.90593992\n",
            " 54.57106838 53.93451983 39.9923124  63.68483843 30.81652934 85.03840799\n",
            " 27.06605433 39.1021461  58.58714625 72.87741955 51.89858709 55.64271978\n",
            " 37.51642569 44.47778615 42.86301781 31.58380185 58.21944024 16.69099961\n",
            " 46.67557337 71.75372813 32.92892668 68.8467312  21.90240249 59.19031866\n",
            " 53.49538436 80.37809723 79.53950539 18.77719703 16.22048329 83.24050753\n",
            " 16.12627449 81.45223822 48.75526263 18.01269919 94.4502277  73.66871218\n",
            " 80.60174321 52.14398902 63.45128737 11.68707799 62.44797137 58.10998598\n",
            " 32.96015291 51.50569231 36.7459804  79.59159312 73.81584649 60.17908711\n",
            " 64.3789354  72.7187817  68.18585866 57.36835573 60.43378683 55.08995901\n",
            " 30.34965244 55.07741631 79.28115825 70.05634918 52.88850451 60.23097622\n",
            " 76.59844208 53.46393021 31.73211521 18.17227287 39.0446084  63.58532077\n",
            " 46.1099699  37.76144649 16.49235634 65.86292873 51.81774436 75.03680873\n",
            " 50.88886976 31.23087582 42.44039212 55.94384579 72.73847104 73.60255679\n",
            " 27.35799624 11.2487884  82.88428081 76.7523301  73.56767318 20.98763269\n",
            " 34.60648399 34.08836595 60.83959954 55.89390818 40.37454096 23.67612739\n",
            " 58.23084835 47.29001706 28.59687462 36.23801424 60.16868111 61.67626562\n",
            " 52.07929351 68.22294092 85.38560718 26.25474567 47.96306904 86.91375043\n",
            " 30.53141429 41.82921026 53.13391957 26.96183073 51.08298407 30.41278418\n",
            " 82.2186313  17.86008377 52.29860802 57.40728702 38.88746213 53.64748737\n",
            " 36.30739054 49.19369608 65.78620698 90.33692754 16.66320772 62.00203591\n",
            " 17.87721653 71.25615055 33.33680123 36.25994257 44.59692499 64.30493636\n",
            " 49.38911025 70.23895243 40.30039201 60.48456289 73.31441135 69.32228045\n",
            " 20.38661661 85.34522482 58.73644185 48.43271035 40.32653715 46.44418062\n",
            " 76.03653378 41.97501051 25.53130567 91.89302981 76.77999633 61.06271829\n",
            " 61.57519721 67.3247383  72.31838312 28.76786969 37.35779252 74.2981476\n",
            " 39.40155574 46.17561046 55.0986801  60.23032469 86.58165766 76.29521094\n",
            " 66.49042207 76.08139157 80.87893746 44.11821476 36.68153688 59.04766714\n",
            " 82.23661197 77.44984784 69.94517185 59.60335615 72.69991201 45.57044781\n",
            " 25.09400743 18.97556828 16.76914265 64.80399326 56.64409893 48.59080305\n",
            " 23.0624257  34.87981619 64.92496075 14.13957357 54.81901186 77.76683456\n",
            " 56.71350039 43.4358615  18.15590316 19.98383524 25.12600529 87.01871306\n",
            " 66.39950367 56.743667   47.32273434 71.63448062 29.51420203 43.26337871\n",
            " 74.98412493 23.08761547 48.95968329 25.3844602  31.75498342 32.68782333\n",
            " 23.93510468 82.75072649 60.40336001 47.44045916 75.2128827  43.01313105\n",
            " 32.5828808  55.07981865 42.04822048 71.15017492 91.85198557 44.80799016\n",
            " 51.38825132 70.93062639 70.83265963 60.81587342 82.91999638 62.72464359\n",
            " 44.49513699 61.55739506 64.86188749 13.55397044 26.30185815  9.05032152\n",
            " 77.86479013 34.71595375 68.20402153 33.13028534 17.1863852  73.76198906\n",
            " 13.58854301 76.35660694 66.72587563 46.19791102 19.43857312 73.566579\n",
            " 57.00069488 75.5875758  43.46151253 68.81943425 71.04335123 69.25105697\n",
            " 33.09305699 41.28955139 44.61222711 58.66664136 42.72113398 23.07698411\n",
            " 70.02172972 69.81265154 83.59274554 43.04348419 54.35089707 74.91684538\n",
            " 68.51468712 48.60320356 92.46413953 19.2012321  60.38061347 38.28015358\n",
            " 75.99711083 59.63573097 54.37779387 52.43522693 81.98724108 73.04833401\n",
            " 42.39343946 24.26318174 91.06372814 36.94639264 37.25689496 69.43764225\n",
            " 32.41128668 68.96031242 18.33200611 24.62377843 36.04917089 51.95607877\n",
            " 23.82023113 74.47177834 52.75550918 66.51284866 66.30323172 43.63803625\n",
            " 21.04943535 44.23899401 29.34373184 52.41203839 24.29825766 57.89711468\n",
            " 60.45469271 76.93973264 70.12634463 22.24216642 31.40420848 72.30171828\n",
            " 59.01159689 26.00947175 43.85951574 24.03613713 70.49176289 92.44062219\n",
            " 49.31895571 74.76783196 37.92407511 50.6804082  59.57808571 35.915284\n",
            " 15.36158828 65.62690753 48.64533187 78.82696188 42.16038545 83.50089224\n",
            " 66.86101849 49.92353174 15.11595766 56.32433923 66.79712474 33.39064801\n",
            " 28.59779611 68.0634667  30.94054516 27.7204051 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг №4 - Метрики (реализация)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sjg9ny2KxzLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Теперь добавим возможность использовать различные метрики для оценки качества модели.\n",
        "\n",
        "#### Задача:\n",
        "\n",
        "1. **Добавьте в класс `MyLineReg` параметр `metric`**, который будет принимать одно из следующих значений:\n",
        "   - `mae` — средняя абсолютная ошибка (Mean Absolute Error).\n",
        "   - `mse` — среднеквадратичная ошибка (Mean Squared Error).\n",
        "   - `rmse` — корень из среднеквадратичной ошибки (Root Mean Squared Error).\n",
        "   - `mape` — средняя абсолютная процентная ошибка (Mean Absolute Percentage Error).\n",
        "   - `r2` — коэффициент детерминации \\(R^2\\).\n",
        "2. По умолчанию: `None`.\n",
        "3. При обучении добавьте в вывод расчёт метрики:\n",
        "\n",
        "Пример вывода:\n",
        "\n",
        "```\n",
        "start | loss: 42027.65 | <metric_name>: 234.65\n",
        "100 | loss: 1222.87 | <metric_name>: 114.35\n",
        "200 | loss: 232.17 | <metric_name>: 58.2\n",
        "300 | loss: 202.4 | <metric_name>: 46.01\n",
        "```\n",
        "\n",
        "4. Если метрика не задана, то ничего дополнительно выводить не нужно.\n",
        "\n",
        "5. **Добавьте метод `get_best_score`**, который возвращает последнее значение метрики (после завершения обучения модели).\n",
        "\n",
        "### Примечания:\n",
        "\n",
        "- Градиентный спуск **всё ещё выполняется по среднеквадратичной ошибке (MSE)**, даже если отслеживаются другие метрики.\n",
        "- Почему добавляем метрику `MSE`, если функция потерь — это уже `MSE`?\n",
        "  - Во-первых, функцией потерь может быть не только `MSE`.\n",
        "  - Во-вторых, мы добавим регуляризацию, которая повлияет на функцию потерь, но **не должна влиять на метрику**."
      ],
      "metadata": {
        "id": "2Bjod6KM6WmQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XAhEZeyw_9Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class MyLineReg:\n",
        "  def __init__(self, n_iter=100, learning_rate=0.1, metric=None, weights=None):\n",
        "    self.n_iter = n_iter\n",
        "    self.learning_rate = learning_rate\n",
        "    self.metric = metric\n",
        "    self.weights = weights\n",
        "    self.best_score = None\n",
        "\n",
        "  def __str__(self):\n",
        "    params = ', '.join(f\"{key}={value}\" for key, value in self.__dict__.items() if key != \"weights\")\n",
        "    return f\"{__class__.__name__} class: {params}\"\n",
        "\n",
        "  def compute_metric(self, y_true, y_pred):\n",
        "    if self.metric == 'mae':\n",
        "      return np.mean(np.abs(y_true - y_pred))\n",
        "    elif self.metric == 'mse':\n",
        "      return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "    elif self.metric == 'rmse':\n",
        "      return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
        "    elif self.metric == 'mape':\n",
        "      return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "    elif self.metric == 'r2':\n",
        "      ss_res = np.sum((y_true - y_pred) ** 2)\n",
        "      ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "      return 1 - (ss_res / ss_tot)\n",
        "    return None\n",
        "\n",
        "  def fit(self, X: pd.DataFrame, y: pd.Series, verbose=False):\n",
        "    # Добавляем единичный столбец для смещения (биаса)\n",
        "    X = np.hstack([np.ones((X.shape[0], 1)), X.values])\n",
        "\n",
        "    # Инициализируем веса единицами\n",
        "    self.weights = np.ones(X.shape[1])\n",
        "\n",
        "    for i in range(1, self.n_iter + 1):\n",
        "      y_pred = X.dot(self.weights)\n",
        "      loss = np.mean((y_pred - y) ** 2)\n",
        "\n",
        "      # Градиент\n",
        "      gradient = (2 / X.shape[0]) * X.T.dot(y_pred - y)\n",
        "\n",
        "      # Обновляем веса\n",
        "      self.weights -= self.learning_rate * gradient\n",
        "\n",
        "      # Логирование на каждой итерации\n",
        "      if verbose and i % verbose == 0:\n",
        "        metric_value = self.compute_metric(y, y_pred)\n",
        "        if self.metric:\n",
        "          print(f\"{i} | loss: {loss:.2f} | {self.metric}: {metric_value:.2f}\")\n",
        "        else:\n",
        "          print(f\"{i} | loss: {loss:.2f}\")\n",
        "\n",
        "    # Вычисляем метрику на последнем шаге после завершения всех итераций\n",
        "    y_pred_final = X.dot(self.weights)  # Предсказания с обновлёнными весами\n",
        "    self.best_score = round(self.compute_metric(y, y_pred_final), 10)\n",
        "\n",
        "  def get_best_score(self):\n",
        "    return self.best_score\n",
        "\n",
        "  # добавляем в наш класс метод предикт\n",
        "  def predict(self, X: pd.DataFrame):\n",
        "    # дополняем матрицу фичей единичным вектором\n",
        "    X = np.hstack([np.ones((X.shape[0], 1)), X.values])\n",
        "    # возвращаем предсказанные значения\n",
        "    return X.dot(self.weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Общий принцип работы кода:\n",
        "1. Инициализация параметров модели.\n",
        "2. Подготовка данных (добавление единичного столбца).\n",
        "3. Итерации градиентного спуска:\n",
        "   - Предсказание текущих значений на основе текущих весов.\n",
        "   - Вычисление ошибки (функции потерь).\n",
        "   - Обновление весов с помощью градиентного спуска.\n",
        "4. Подсчет и вывод выбранной метрики (если она указана).\n",
        "5. Возвращение результата.\n",
        "\n",
        "Теперь разберем это по шагам.\n",
        "\n",
        "#### 1. **Инициализация параметров модели**\n",
        "Когда ты создаешь объект класса `MyLineReg`, запускается конструктор:\n",
        "\n",
        "```python\n",
        "class MyLineReg:\n",
        "    def __init__(self, n_iter=100, learning_rate=0.1, metric=None, weights=None):\n",
        "        self.n_iter = n_iter\n",
        "        self.learning_rate = learning_rate\n",
        "        self.metric = metric\n",
        "        self.weights = weights\n",
        "        self.best_score = None\n",
        "```\n",
        "\n",
        "- **`n_iter`**: Сколько раз ты будешь обновлять веса (число итераций).\n",
        "- **`learning_rate`**: Насколько сильно изменяются веса на каждой итерации (скорость обучения).\n",
        "- **`metric`**: Какая метрика будет использоваться для отслеживания качества модели.\n",
        "- **`weights`**: Веса модели (они пока не заданы, это будет сделано позже).\n",
        "- **`best_score`**: Сохраняется финальный результат выбранной метрики после завершения обучения.\n",
        "\n",
        "_Этот шаг выполняется, когда ты создаешь объект модели, например:_\n",
        "```python\n",
        "model = MyLineReg(n_iter=300, learning_rate=0.05, metric='mae')\n",
        "```\n",
        "\n",
        "#### 2. **Подготовка данных**\n",
        "Данные, с которыми ты работаешь, состоят из признаков $X$ и целевой переменной $y$. Для работы модели нам нужно:\n",
        "- Добавить единичный столбец к признакам $X$, чтобы учесть смещение (bias).\n",
        "- Инициализировать веса.\n",
        "\n",
        "```python\n",
        "X = np.hstack([np.ones((X.shape[0], 1)), X.values])\n",
        "self.weights = np.ones(X.shape[1])\n",
        "```\n",
        "\n",
        "##### Подробное описание:\n",
        "- **`X.shape[0]`**: Число строк в матрице признаков (количество примеров).\n",
        "- **`np.hstack`**: Добавляет столбец единиц к $X$. Это нужно для учёта смещения. Теперь $X$ выглядит так:\n",
        "\n",
        "$$\n",
        "X =\n",
        "\\begin{pmatrix}\n",
        "1 & x_{11} & x_{12} & ... & x_{1n} \\\\\n",
        "1 & x_{21} & x_{22} & ... & x_{2n} \\\\\n",
        "\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n",
        "1 & x_{m1} & x_{m2} & ... & x_{mn}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Каждый пример данных теперь имеет дополнительную фиктивную (единичную) переменную, которая соответствует смещению $b$ в модели.\n",
        "\n",
        "- **Инициализация весов**: Веса (коэффициенты регрессии) инициализируются единицами, по одному весу на каждый признак. Если $X$ имеет $n$ признаков, веса будут:\n",
        "\n",
        "$$\n",
        "W = [1, 1, 1, ... , 1]\n",
        "$$\n",
        "\n",
        "#### 3. **Итерации градиентного спуска**\n",
        "Теперь начинается ключевая часть — итерации градиентного спуска. Это цикл, в котором на каждом шаге происходят следующие действия:\n",
        "\n",
        "##### 3.1. **Предсказание**\n",
        "```python\n",
        "y_pred = X.dot(self.weights)\n",
        "```\n",
        "На каждой итерации ты используешь текущие веса для предсказания $y_{pred}$, используя формулу:\n",
        "\n",
        "$$\n",
        "y_{pred} = X \\cdot W\n",
        "$$\n",
        "\n",
        "- **`X.dot(self.weights)`**: Умножает матрицу признаков $X$ на текущий вектор весов $W$. Это предсказание значений целевой переменной $y_{pred}$.\n",
        "\n",
        "##### Пример:\n",
        "Допустим, у тебя есть 2 признака и 3 примера:\n",
        "\n",
        "$$\n",
        "X =\n",
        "\\begin{pmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "1 & 4 & 5 \\\\\n",
        "1 & 6 & 7\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "Текущие веса:\n",
        "\n",
        "$$\n",
        "W = [1, 0.5, -0.5]\n",
        "$$\n",
        "Предсказания:\n",
        "\n",
        "$$\n",
        "y_{pred} = X \\cdot W = [1 \\cdot 1 + 2 \\cdot 0.5 + 3 \\cdot (-0.5), 1 \\cdot 1 + 4 \\cdot 0.5 + 5 \\cdot (-0.5), 1 \\cdot 1 + 6 \\cdot 0.5 + 7 \\cdot (-0.5)] = [0.5, 0.5, 0.5]\n",
        "$$\n",
        "\n",
        "##### 3.2. **Вычисление функции потерь (MSE)**\n",
        "```python\n",
        "loss = np.mean((y_pred - y) ** 2)\n",
        "```\n",
        "Здесь вычисляется среднеквадратичная ошибка (MSE):\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^m (y_{pred_i} - y_i)^2\n",
        "$$\n",
        "Где $m$ — количество примеров в данных, $y_i$ — реальные значения, а $y_{pred_i}$ — предсказанные.\n",
        "\n",
        "##### 3.3. **Вычисление градиента**\n",
        "```python\n",
        "gradient = (2 / X.shape[0]) * X.T.dot(y_pred - y)\n",
        "```\n",
        "\n",
        "- **Градиент** — это производная функции потерь по весам. Он показывает направление, в котором нужно двигаться, чтобы уменьшить ошибку.\n",
        "\n",
        "$$\n",
        "\\text{Градиент} = \\frac{2}{m} X^T \\cdot (y_{pred} - y)\n",
        "$$\n",
        "\n",
        "- **`X.T.dot(y_pred - y)`**: Сначала вычисляется разница между предсказанными и реальными значениями ($y_{pred} - y$), затем умножается на транспонированную матрицу признаков $X^T$. Это даёт вектор, который указывает, как сильно нужно изменить каждый вес, чтобы уменьшить ошибку.\n",
        "\n",
        "##### 3.4. **Обновление весов**\n",
        "```python\n",
        "self.weights -= self.learning_rate * gradient\n",
        "```\n",
        "Теперь мы обновляем веса:\n",
        "\n",
        "$$\n",
        "W = W - \\alpha \\cdot \\text{Градиент}\n",
        "$$\n",
        "\n",
        "Где:\n",
        "- $\\alpha$ — скорость обучения (learning rate),\n",
        "- **Градиент** — направление, в котором нужно обновить веса.\n",
        "\n",
        "Это шаг градиентного спуска. Веса изменяются, чтобы на следующем шаге уменьшить функцию потерь.\n",
        "\n",
        "#### 4. **Подсчет метрики и логирование**\n",
        "```python\n",
        "metric_value = self.compute_metric(y, y_pred)\n",
        "```\n",
        "\n",
        "В конце каждой итерации (или через каждые несколько итераций, если используется `verbose`), ты вычисляешь выбранную метрику (например, MAE, MSE и т.д.). Она позволяет отслеживать качество модели в процессе обучения.\n",
        "\n",
        "#### 5. **Конец обучения**\n",
        "После завершения всех итераций модель сохраняет последнее значение метрики:\n",
        "\n",
        "```python\n",
        "self.best_score = round(self.compute_metric(y, y_pred_final), 10)\n",
        "```\n",
        "\n",
        "Теперь модель завершила обучение, и ты можешь использовать метод `get_best_score`, чтобы получить финальную метрику.\n",
        "\n",
        "#### Визуализация работы кода в голове\n",
        "\n",
        "1. **Инициализация**:\n",
        "   - Модель начинает с нулевых весов (всех единиц).\n",
        "   - Данные $X$ дополняются единичным столбцом для учёта смещения.\n",
        "\n",
        "2. **Итерации обучения**:\n",
        "   - Каждый шаг итерации можно представить как «движение» в пространстве весов, где модель постепенно находит такие значения весов, которые минимизируют ошибку.\n",
        "   - На каждом шаге модель делает предсказания, вычисляет ошибку, затем обновляет веса в направлении уменьшения этой ошибки.\n",
        "\n",
        "3. **Финальная метрика**:\n",
        "   - Когда обучение завершено, модель готова делать финальные предсказания с улучшенными весами. Метрика на последнем шаге оценивает, насколько хорошо модель справляется с задачей."
      ],
      "metadata": {
        "id": "DoVYMYgYwowj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг №5 - Регуляризация (реализация)\n",
        "\n"
      ],
      "metadata": {
        "id": "TrKhx0C8x5_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Добавьте в класс `MyLineReg` три параметра:\n",
        "\n",
        "- `reg` – принимает одно из трех значений: `'l1'`, `'l2'`, `'elasticnet'`.  \n",
        "  По умолчанию: `None`.\n",
        "- `l1_coef` – принимает значения от 0.0 до 1.0.  \n",
        "  По умолчанию: `0`.\n",
        "- `l2_coef` – принимает значения от 0.0 до 1.0.  \n",
        "  По умолчанию: `0`.\n",
        "\n",
        "Добавьте регуляризацию к вычислению лосса.  \n",
        "Добавьте регуляризацию к вычислению градиента.\n",
        "\n",
        "#### Примечания:\n",
        "\n",
        "- Для вычисления регуляризации L1 вам нужно задать `reg=\"l1\"` и указать только `l1_coef`.\n",
        "- Для вычисления L2 вам нужно задать `reg=\"l2\"` и указать только `l2_coef`.\n",
        "- Для вычисления Elasticnet вам нужно задать `reg=\"elasticnet\"` и указать оба параметра `l1_coef` и `l2_coef`.\n",
        "\n",
        "### Проверка\n",
        "\n",
        "**Входные данные**: три вида регуляризации и одна модель без регуляризации (`None`).  \n",
        "**Выходные данные**: коэффициенты обученной линейной регрессии (их сумма)."
      ],
      "metadata": {
        "id": "C-p2FZDq6huA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(MyLineReg())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fuUqwfaYZhP",
        "outputId": "2356e5ec-9212-450a-c91b-d5d43caed829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyLineReg class: n_iter=100, learning_rate=0.1, metric=None, best_score=None, reg=None, l1_coef=0.0, l2_coef=0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class MyLineReg:\n",
        "    def __init__(self,\n",
        "                 n_iter=100,\n",
        "                 learning_rate=0.1,\n",
        "                 metric=None,\n",
        "                 weights=None,\n",
        "                 reg=None,\n",
        "                 l1_coef=0.0,\n",
        "                 l2_coef=0.0):\n",
        "\n",
        "      self.n_iter = n_iter\n",
        "      self.learning_rate = learning_rate\n",
        "      self.metric = metric\n",
        "      self.weights = weights\n",
        "      self.best_score = None\n",
        "      self.reg = reg  # Регуляризация ('l1', 'l2', 'elasticnet', или None)\n",
        "      self.l1_coef = l1_coef  # Коэффициент для L1 регуляризации\n",
        "      self.l2_coef = l2_coef  # Коэффициент для L2 регуляризации\n",
        "\n",
        "    def __str__(self):\n",
        "        params = ', '.join(f\"{key}={value}\" for key, value in self.__dict__.items() if key != \"weights\")\n",
        "        return f\"{__class__.__name__} class: {params}\"\n",
        "\n",
        "    def compute_metric(self, y_true, y_pred):\n",
        "        if self.metric == 'mae':\n",
        "            return np.mean(np.abs(y_true - y_pred))\n",
        "        elif self.metric == 'mse':\n",
        "            return np.mean((y_true - y_pred) ** 2)\n",
        "        elif self.metric == 'rmse':\n",
        "            return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
        "        elif self.metric == 'mape':\n",
        "            return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "        elif self.metric == 'r2':\n",
        "            ss_res = np.sum((y_true - y_pred) ** 2)\n",
        "            ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "            return 1 - (ss_res / ss_tot)\n",
        "        return None\n",
        "\n",
        "    def compute_loss(self, y, y_pred):\n",
        "        # Основной лосс — MSE\n",
        "        loss = np.mean((y_pred - y) ** 2)\n",
        "\n",
        "        # Добавляем регуляризацию к лоссу\n",
        "        if self.reg == 'l1':\n",
        "            loss += self.l1_coef * np.sum(np.abs(self.weights))  # L1 регуляризация\n",
        "        elif self.reg == 'l2':\n",
        "            loss += self.l2_coef * np.sum(self.weights ** 2)  # L2 регуляризация\n",
        "        elif self.reg == 'elasticnet':\n",
        "            loss += self.l1_coef * np.sum(np.abs(self.weights)) + self.l2_coef * np.sum(self.weights ** 2)  # ElasticNet\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def compute_gradient(self, X, y, y_pred):\n",
        "        # Обычный градиент\n",
        "        gradient = (2 / X.shape[0]) * X.T.dot(y_pred - y)\n",
        "\n",
        "        # Добавляем регуляризацию к градиенту\n",
        "        if self.reg == 'l1':\n",
        "            gradient += self.l1_coef * np.sign(self.weights)  # Для L1 добавляем знак весов\n",
        "        elif self.reg == 'l2':\n",
        "            gradient += 2 * self.l2_coef * self.weights  # Для L2 градиент добавляется как 2 * lambda * w\n",
        "        elif self.reg == 'elasticnet':\n",
        "            gradient += self.l1_coef * np.sign(self.weights) + 2 * self.l2_coef * self.weights  # Для ElasticNet\n",
        "\n",
        "        return gradient\n",
        "\n",
        "    def fit(self, X: pd.DataFrame, y: pd.Series, verbose=False):\n",
        "        # Добавляем единичный столбец для смещения (биаса)\n",
        "        X = np.hstack([np.ones((X.shape[0], 1)), X.values])\n",
        "\n",
        "        # Инициализируем веса единицами\n",
        "        self.weights = np.ones(X.shape[1])\n",
        "\n",
        "        for i in range(1, self.n_iter + 1):\n",
        "            y_pred = X.dot(self.weights)\n",
        "            loss = self.compute_loss(y, y_pred)\n",
        "\n",
        "            # Градиент\n",
        "            gradient = self.compute_gradient(X, y, y_pred)\n",
        "\n",
        "            # Обновляем веса\n",
        "            self.weights -= self.learning_rate * gradient\n",
        "\n",
        "            # Логирование на каждой итерации\n",
        "            if verbose and i % verbose == 0:\n",
        "                metric_value = self.compute_metric(y, y_pred)\n",
        "                if self.metric:\n",
        "                    print(f\"{i} | loss: {loss:.6f} | {self.metric}: {metric_value:.6f}\")\n",
        "                else:\n",
        "                    print(f\"{i} | loss: {loss:.6f}\")\n",
        "\n",
        "        # Вычисляем метрику на последнем шаге после завершения всех итераций\n",
        "        y_pred_final = X.dot(self.weights)  # Предсказания с обновлёнными весами\n",
        "        self.best_score = round(self.compute_metric(y, y_pred_final), 10)\n",
        "\n",
        "    def get_best_score(self):\n",
        "        return self.best_score\n",
        "\n",
        "    # Метод для получения коэффициентов (весов)\n",
        "    def get_coef(self):\n",
        "        return self.weights\n",
        "\n",
        "    # Метод для предсказания на новых данных\n",
        "    def predict(self, X: pd.DataFrame):\n",
        "        # Дополняем матрицу фичей единичным вектором\n",
        "        X = np.hstack([np.ones((X.shape[0], 1)), X.values])\n",
        "        # Возвращаем предсказанные значения\n",
        "        return X.dot(self.weights)\n"
      ],
      "metadata": {
        "id": "jYfGsiJfsTdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг №6 - Скорость обучения"
      ],
      "metadata": {
        "id": "aYExSS1g6iwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "параметр learning rate теперь будет определяться callable"
      ],
      "metadata": {
        "id": "DeVvlskx6plr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Возьмите код из предыдущего шага и модифицируйте в нем параметр learning_rate следующим:\n",
        "\n",
        "Если на вход пришло число, то работаем как и раньше.\n",
        "Если на вход пришла lambda-функция, то вычисляем learning_rate на каждом шаге на основе переданной лямбда-функции.\n",
        "Можете дополнительно для контроля вывести значение learning_rate в лог тренировки.\n",
        "\n",
        "Примечания:\n",
        "\n",
        "Т.к. у нас теперь результат зависит от нумерации шагов, то формализуем их нумерацию: они должна считаться от 1 до n_iter (включительно)."
      ],
      "metadata": {
        "id": "_Dj4sv06EAdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ключевая часть кода, в которой были произведены изменения\n",
        "\n",
        "            # Вычисление шага обучения\n",
        "            if callable(self.learning_rate):\n",
        "                lr = self.learning_rate(i)  # Используем лямбда-функцию для вычисления шага\n",
        "            else:\n",
        "                lr = self.learning_rate\n",
        "\n",
        "            # Градиент\n",
        "            gradient = self.compute_gradient(X, y, y_pred)\n",
        "\n",
        "            # Обновляем веса\n",
        "            self.weights -= lr * gradient"
      ],
      "metadata": {
        "id": "bOlI8_U396aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class MyLineReg:\n",
        "    def __init__(self, n_iter=100, learning_rate=0.1, metric=None, weights=None, reg=None, l1_coef=0.0, l2_coef=0.0):\n",
        "        self.n_iter = n_iter\n",
        "        self.learning_rate = learning_rate\n",
        "        self.metric = metric\n",
        "        self.weights = weights\n",
        "        self.best_score = None\n",
        "        self.reg = reg  # Регуляризация (None, 'l1', 'l2', 'elasticnet')\n",
        "        self.l1_coef = l1_coef  # Коэффициент для L1 регуляризации\n",
        "        self.l2_coef = l2_coef  # Коэффициент для L2 регуляризации\n",
        "\n",
        "    def __str__(self):\n",
        "        params = ', '.join(f\"{key}={value}\" for key, value in self.__dict__.items() if key != \"weights\")\n",
        "        return f\"{__class__.__name__} class: {params}\"\n",
        "\n",
        "    def compute_metric(self, y_true, y_pred):\n",
        "        if self.metric == 'mae':\n",
        "            return np.mean(np.abs(y_true - y_pred))\n",
        "        elif self.metric == 'mse':\n",
        "            return np.mean((y_true - y_pred) ** 2)\n",
        "        elif self.metric == 'rmse':\n",
        "            return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
        "        elif self.metric == 'mape':\n",
        "            return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "        elif self.metric == 'r2':\n",
        "            ss_res = np.sum((y_true - y_pred) ** 2)\n",
        "            ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "            return 1 - (ss_res / ss_tot)\n",
        "        return None  # Возвращает None, если метрика не задана или не поддерживается\n",
        "\n",
        "    def compute_loss(self, y, y_pred):\n",
        "        # Обычный MSE Loss\n",
        "        loss = np.mean((y_pred - y) ** 2)\n",
        "        # Добавляем регуляризацию к лоссу\n",
        "        if self.reg == 'l1':  # L1 регуляризация\n",
        "            loss += self.l1_coef * np.sum(np.abs(self.weights))  # Не применяем регуляризацию к bias (весам смещения)\n",
        "        elif self.reg == 'l2':  # L2 регуляризация\n",
        "            loss += self.l2_coef * np.sum(self.weights ** 2)\n",
        "        elif self.reg == 'elasticnet':  # ElasticNet регуляризация\n",
        "            loss += self.l1_coef * np.sum(np.abs(self.weights)) + self.l2_coef * np.sum(self.weights ** 2)\n",
        "        return loss\n",
        "\n",
        "    def compute_gradient(self, X, y, y_pred):\n",
        "        # Обычный градиент\n",
        "        gradient = (2 / X.shape[0]) * X.T.dot(y_pred - y)\n",
        "        # Добавляем регуляризацию к градиенту\n",
        "        if self.reg == 'l1':  # L1 регуляризация (только на весах, не на смещении)\n",
        "            gradient += self.l1_coef * np.sign(self.weights)\n",
        "        elif self.reg == 'l2':  # L2 регуляризация\n",
        "            gradient += 2 * self.l2_coef * self.weights\n",
        "        elif self.reg == 'elasticnet':  # ElasticNet регуляризация\n",
        "            gradient += self.l1_coef * np.sign(self.weights) + 2 * self.l2_coef * self.weights\n",
        "        return gradient\n",
        "\n",
        "    def fit(self, X: pd.DataFrame, y: pd.Series, verbose=False):\n",
        "        # Добавляем единичный столбец для смещения (биаса)\n",
        "        X = np.hstack([np.ones((X.shape[0], 1)), X.values])\n",
        "        # Инициализируем веса единицами\n",
        "        self.weights = np.ones(X.shape[1])\n",
        "        for i in range(1, self.n_iter + 1):\n",
        "            y_pred = X.dot(self.weights)\n",
        "            loss = self.compute_loss(y, y_pred)\n",
        "            # Вычисление шага обучения\n",
        "            if callable(self.learning_rate):\n",
        "                lr = self.learning_rate(i)  # Используем лямбда-функцию для вычисления шага\n",
        "            else:\n",
        "                lr = self.learning_rate\n",
        "            # Градиент\n",
        "            gradient = self.compute_gradient(X, y, y_pred)\n",
        "            # Обновляем веса\n",
        "            self.weights -= lr * gradient\n",
        "            # Логирование на каждой итерации\n",
        "            if verbose and i % verbose == 0:\n",
        "                metric_value = self.compute_metric(y, y_pred)\n",
        "                if self.metric:\n",
        "                    print(f\"{i} | loss: {loss:.6f} | learning_rate: {lr:.6f}\")\n",
        "                else:\n",
        "                    print(f\"{self.metric}: {metric_value:.6f}\")\n",
        "        # Вычисляем метрику на последнем шаге после завершения всех итераций\n",
        "        y_pred_final = X.dot(self.weights)  # Предсказания с обновлёнными весами\n",
        "        # Вычисляем лучшую метрику, если она задана, иначе присваиваем значение None\n",
        "        final_metric_value = self.compute_metric(y, y_pred_final)\n",
        "        if final_metric_value is not None:\n",
        "            self.best_score = round(final_metric_value, 10)\n",
        "        else:\n",
        "            self.best_score = None\n",
        "\n",
        "    def get_best_score(self):\n",
        "        return self.best_score\n",
        "\n",
        "    # Новый метод для получения коэффициентов (весов)\n",
        "    def get_coef(self):\n",
        "        # Возвращаем все веса, кроме первого элемента (bias)\n",
        "        return self.weights[1:]\n",
        "\n",
        "    # Метод для предсказания на новых данных\n",
        "    def predict(self, X: pd.DataFrame):\n",
        "        # Дополняем матрицу фичей единичным вектором\n",
        "        X = np.hstack([np.ones((X.shape[0], 1)), X.values])\n",
        "        # Возвращаем предсказанные значения\n",
        "        return X.dot(self.weights)"
      ],
      "metadata": {
        "id": "sOTwZmzu6qGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг №7 - Стохастический градиентный спуск (реализация)"
      ],
      "metadata": {
        "id": "dQMGhx8b6qvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Добавьте в класс MyLineReg два новых параметра:\n",
        "\n",
        "- sgd_sample – кол-во образцов, которое будет использоваться на каждой итерации обучения. Может принимать либо целые числа, либо дробные от 0.0 до 1.0.\n",
        "По-умолчанию: None\n",
        "- random_state – для воспроизводимости результата зафиксируем сид (об этом далее).\n",
        "По-умолчанию: 42\n",
        "\n",
        "Внесем изменение в алгоритм обучения:\n",
        "\n",
        "- В начале обучения фиксируем сид (см. ниже).\n",
        "- В начале каждого шага формируется новый мини-пакет, состоящий из случайно выбранных элементов обучающего набора. Кол-во отобранных элементов определяется параметром sgd_sample:\n",
        "  - Если задано целое число, то из исходного датасета берется ровно столько примеров сколько указано.\n",
        "  - Если задано дробное число, то рассматриваем его как долю от количества строк в исходном датасете (округленное до целого числа).\n",
        "- Расчет градиента (и последующее изменение весов) делаем на основе мини-пакета.\n",
        "- Все остальные параметры, если они заданы (например, регуляризация), также должны учитываться при обучении.\n",
        "- Ошибку и метрику необходимо считать на всем датасете, а не на мини-пакете.\n",
        "- Если sgd_sample = None, то обучение выполняется как раньше (на всех данных)."
      ],
      "metadata": {
        "id": "wwUg_y-L6ql-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Случайная генерация\n",
        "\n",
        "Т.к. у нас формальная проверка кода, то у всех должны получиться одинаковые случайные подвыборки. Поэтому и способ у всех будет одинаковый.\n",
        "\n",
        "В начале обучения посредством модуля random фиксируем сид:\n",
        "\n",
        "```random.seed(<random_state>)```\n",
        "\n",
        "В начале каждой итерации сформируем порядковые номера строк, которые стоит отобрать.\n",
        "\n",
        "```sample_rows_idx = random.sample(range(X.shape[0]), <sgd_sample>)```\n",
        "\n",
        "В этом случае при каждом запуске будут генерироваться одни и те же номера строк. Что позволит нам добиться воспроизводимости.\n",
        "\n",
        "З.Ы. Модуль random уже импортирован.\n",
        "\n",
        "З.Ы.2. При отборе строк не стоит полагаться на индекс пандаса – он может быть не последовательным."
      ],
      "metadata": {
        "id": "DyyibCrFRsmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "class MyLineReg:\n",
        "    def __init__(self,\n",
        "                 n_iter=100,\n",
        "                 learning_rate=0.1,\n",
        "                 metric=None,\n",
        "                 weights=None,\n",
        "                 reg=None,\n",
        "                 l1_coef=0.0,\n",
        "                 l2_coef=0.0,\n",
        "                 sgd_sample=None,\n",
        "                 random_state=42):\n",
        "        self.n_iter = n_iter\n",
        "        self.learning_rate = learning_rate\n",
        "        self.metric = metric\n",
        "        self.weights = weights\n",
        "        self.best_score = None\n",
        "        self.reg = reg  # Регуляризация (None, 'l1', 'l2', 'elasticnet')\n",
        "        self.l1_coef = l1_coef  # Коэффициент для L1 регуляризации\n",
        "        self.l2_coef = l2_coef  # Коэффициент для L2 регуляризации\n",
        "        self.sgd_sample = sgd_sample # Количество выборок для SGD\n",
        "        self.random_state = random_state  # Для воспроизводимости\n",
        "\n",
        "    def __str__(self):\n",
        "        params = ', '.join(f\"{key}={value}\" for key, value in self.__dict__.items() if key != \"weights\")\n",
        "        return f\"{__class__.__name__} class: {params}\"\n",
        "\n",
        "    def compute_metric(self, y_true, y_pred):\n",
        "        if self.metric == 'mae':\n",
        "            return np.mean(np.abs(y_true - y_pred))\n",
        "        elif self.metric == 'mse':\n",
        "            return np.mean((y_true - y_pred) ** 2)\n",
        "        elif self.metric == 'rmse':\n",
        "            return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
        "        elif self.metric == 'mape':\n",
        "            return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "        elif self.metric == 'r2':\n",
        "            ss_res = np.sum((y_true - y_pred) ** 2)\n",
        "            ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "            return 1 - (ss_res / ss_tot)\n",
        "        return None  # Возвращает None, если метрика не задана или не поддерживается\n",
        "\n",
        "    def compute_loss(self, y, y_pred):\n",
        "        # Обычный MSE Loss\n",
        "        loss = np.mean((y_pred - y) ** 2)\n",
        "        # Добавляем регуляризацию к лоссу\n",
        "        if self.reg == 'l1':  # L1 регуляризация\n",
        "            loss += self.l1_coef * np.sum(np.abs(self.weights))  # Не применяем регуляризацию к bias (весам смещения)\n",
        "        elif self.reg == 'l2':  # L2 регуляризация\n",
        "            loss += self.l2_coef * np.sum(self.weights ** 2)\n",
        "        elif self.reg == 'elasticnet':  # ElasticNet регуляризация\n",
        "            loss += self.l1_coef * np.sum(np.abs(self.weights)) + self.l2_coef * np.sum(self.weights ** 2)\n",
        "        return loss\n",
        "\n",
        "    def compute_gradient(self, X, y, y_pred):\n",
        "        # Обычный градиент\n",
        "        gradient = (2 / X.shape[0]) * X.T.dot(y_pred - y)\n",
        "        # Добавляем регуляризацию к градиенту\n",
        "        if self.reg == 'l1':  # L1 регуляризация (только на весах, не на смещении)\n",
        "            gradient += self.l1_coef * np.sign(self.weights)\n",
        "        elif self.reg == 'l2':  # L2 регуляризация\n",
        "            gradient += 2 * self.l2_coef * self.weights\n",
        "        elif self.reg == 'elasticnet':  # ElasticNet регуляризация\n",
        "            gradient += self.l1_coef * np.sign(self.weights) + 2 * self.l2_coef * self.weights\n",
        "        return gradient\n",
        "\n",
        "    def fit(self, X: pd.DataFrame, y: pd.Series, verbose=False):\n",
        "        # фиксируем сид для воспроизводимости\n",
        "        random.seed(self.random_state)\n",
        "        # Добавляем единичный столбец для смещения (биаса)\n",
        "        X = np.hstack([np.ones((X.shape[0], 1)), X.values])\n",
        "        # Инициализируем веса единицами\n",
        "        self.weights = np.ones(X.shape[1])\n",
        "        # Определяем кол-во примеров для мини-батча\n",
        "        if self.sgd_sample is not None:\n",
        "          if isinstance(self.sgd_sample, float) and 0 < self.sgd_sample <= 1:\n",
        "            batch_size = int(self.sgd_sample * X.shape[0])  # Интерпретируем как долю от данных\n",
        "          elif isinstance(selg.sgd_sample, int) and self.sgd_sample > 0:\n",
        "            batch_size = self.sgd_sample  # Определенное количество строк\n",
        "          else:\n",
        "            raise VelueError(\"sgd_sample должен быть целым числом или дробным значением от 0 до 1.\")\n",
        "        else:\n",
        "          batch_size = X.shape[0]  # Если не задано, то берем все данные\n",
        "        for i in range(1, self.n_iter + 1):\n",
        "            # Формируем мини батч\n",
        "            if batch_size < X.shape[0]:\n",
        "              sample_rows_idx = random.sample(range(X.shape[0]), batch_size)\n",
        "              X_batch = X[sample_rows_idx]\n",
        "              y_batch = y.values[sample_rows_idx]\n",
        "            else:\n",
        "              X_batch = X\n",
        "              y_batch = y\n",
        "            # Предсказания для мини-батча\n",
        "            y_pred_batch = X_batch.dot(self.weights)\n",
        "            # Градиент\n",
        "            gradient = self.compute_gradient(X_batch, y_batch, y_pred_batch)\n",
        "            # Вычисление шага обучения\n",
        "            if callable(self.learning_rate):\n",
        "              lr = self.learning_rate(i)  # Используем лямбда-функцию для вычисления шага\n",
        "            else:\n",
        "              lr = self.learning_rate\n",
        "            # Обновляем веса\n",
        "            self.weights -= lr*gradient\n",
        "            # Полное предсказание всех данных и вычисление метрики/лосса\n",
        "            # на всех данных\n",
        "            y_pred = X.dot(self.weights)\n",
        "            loss = self.compute_loss(y, y_pred)\n",
        "            # Логирование на каждой итерации\n",
        "            if verbose and i % verbose == 0:\n",
        "                metric_value = self.compute_metric(y, y_pred)\n",
        "                if self.metric:\n",
        "                    print(f\"{i} | loss: {loss:.6f} | learning_rate: {lr:.6f}\")\n",
        "                else:\n",
        "                    print(f\"{self.metric}: {metric_value:.6f}\")\n",
        "        # Вычисляем метрику на последнем шаге после завершения всех итераций\n",
        "        y_pred_final = X.dot(self.weights)  # Предсказания с обновлёнными весами\n",
        "        # Вычисляем лучшую метрику, если она задана, иначе присваиваем значение None\n",
        "        final_metric_value = self.compute_metric(y, y_pred_final)\n",
        "        if final_metric_value is not None:\n",
        "            self.best_score = round(final_metric_value, 10)\n",
        "        else:\n",
        "            self.best_score = None\n",
        "\n",
        "    def get_best_score(self):\n",
        "        return self.best_score\n",
        "\n",
        "    # Новый метод для получения коэффициентов (весов)\n",
        "    def get_coef(self):\n",
        "        # Возвращаем все веса, кроме первого элемента (bias)\n",
        "        return self.weights[1:]\n",
        "\n",
        "    # Метод для предсказания на новых данных\n",
        "    def predict(self, X: pd.DataFrame):\n",
        "        # Дополняем матрицу фичей единичным вектором\n",
        "        X = np.hstack([np.ones((X.shape[0], 1)), X.values])\n",
        "        # Возвращаем предсказанные значения\n",
        "        return X.dot(self.weights)"
      ],
      "metadata": {
        "id": "0OpZPF1a6rAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг №8 - Тестируем получившийся класс"
      ],
      "metadata": {
        "id": "lCi94TGbegce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Синтетические данные"
      ],
      "metadata": {
        "id": "1cgsTXgCejdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Функция для генерации синтетических данных\n",
        "def generate_data(n_samples=1000, n_features=3, noise=0.1, random_state=42):\n",
        "    np.random.seed(random_state)  # Фиксируем сид для воспроизводимости\n",
        "    random.seed(random_state)\n",
        "\n",
        "    # Генерируем случайные коэффициенты для линейной модели\n",
        "    true_weights = np.random.randn(n_features)\n",
        "    bias = np.random.randn()  # Генерируем смещение (bias)\n",
        "\n",
        "    # Генерируем случайные данные для признаков\n",
        "    X = np.random.randn(n_samples, n_features)\n",
        "\n",
        "    # Генерируем целевые значения с добавлением случайного шума\n",
        "    y = X.dot(true_weights) + bias + noise * np.random.randn(n_samples)\n",
        "\n",
        "    # Преобразуем данные в DataFrame для удобства\n",
        "    X_df = pd.DataFrame(X, columns=[f'feature_{i+1}' for i in range(n_features)])\n",
        "    y_series = pd.Series(y, name='target')\n",
        "\n",
        "    return X_df, y_series, true_weights, bias\n",
        "\n",
        "# Генерация данных\n",
        "X_train, y_train, true_weights, true_bias = generate_data(n_samples=100, n_features=3, noise=0.1)\n",
        "\n",
        "# Вывод истинных коэффициентов и смещения\n",
        "print(f\"True coefficients (weights): {true_weights}\")\n",
        "print(f\"True bias: {true_bias}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIzq7kt-WoX0",
        "outputId": "1ab190c3-0f25-451b-dcb0-881dfe0b43ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True coefficients (weights): [ 0.49671415 -0.1382643   0.64768854]\n",
            "True bias: 1.5230298564080254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyLineReg(n_iter=1000, learning_rate=0.1, metric='mse', sgd_sample=0.1, random_state=42)\n",
        "model.fit(X_train, y_train, verbose=100)\n",
        "learned_weights = model.get_coef()\n",
        "print(f\"Learned coefficients: {learned_weights}\")\n",
        "print(f\"True coefficients: {true_weights}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JP799iIeuVi",
        "outputId": "c2eb47be-35e1-4509-c81b-d33cb1ecbd6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 | loss: 0.007748 | learning_rate: 0.100000\n",
            "200 | loss: 0.008134 | learning_rate: 0.100000\n",
            "300 | loss: 0.007689 | learning_rate: 0.100000\n",
            "400 | loss: 0.007626 | learning_rate: 0.100000\n",
            "500 | loss: 0.007683 | learning_rate: 0.100000\n",
            "600 | loss: 0.007876 | learning_rate: 0.100000\n",
            "700 | loss: 0.007558 | learning_rate: 0.100000\n",
            "800 | loss: 0.007847 | learning_rate: 0.100000\n",
            "900 | loss: 0.007830 | learning_rate: 0.100000\n",
            "1000 | loss: 0.007656 | learning_rate: 0.100000\n",
            "Learned coefficients: [ 0.52060662 -0.14627314  0.63750582]\n",
            "True coefficients: [ 0.49671415 -0.1382643   0.64768854]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг №9 - Пояснения с примерами того, как работает линейная регрессия от ChatGPT. Для наглядности"
      ],
      "metadata": {
        "id": "JgCizeSmMoAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давай детально разберем метод `fit` и его математические выкладки.\n",
        "\n",
        "### Математические выкладки метода `fit`\n",
        "\n",
        "#### 1. **Постановка задачи**\n",
        "Метод `fit` реализует обучение линейной модели, цель которой — найти набор весов **w** и смещение **b**, которые минимизируют функцию потерь (например, среднеквадратическую ошибку, MSE) на обучающей выборке.\n",
        "\n",
        "**Линейная модель** имеет следующий вид:\n",
        "\n",
        "$$\n",
        "\\hat{y} = X \\cdot w + b\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $X$ — это матрица признаков, каждая строка соответствует одному объекту, каждая колонка — признаку,\n",
        "- $w$ — это вектор весов (коэффициентов) для признаков,\n",
        "- $b$ — смещение (bias),\n",
        "- $\\hat{y}$ — предсказанные значения целевой переменной.\n",
        "\n",
        "#### 2. **Функция потерь**\n",
        "\n",
        "Часто используется **MSE** (mean squared error, среднеквадратическая ошибка), которая вычисляется по формуле:\n",
        "\n",
        "$\n",
        "L(w, b) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "$\n",
        "\n",
        "где:\n",
        "- $y_i$ — истинное значение целевой переменной,\n",
        "- $\\hat{y}_i$ — предсказанное значение,\n",
        "- $n$ — количество обучающих примеров.\n",
        "\n",
        "Добавляется регуляризация:\n",
        "- **L1-регуляризация (Lasso):**\n",
        "$$\n",
        "L(w) = \\lambda_1 \\sum_{j=1}^{p} |w_j|\n",
        "$$\n",
        "- **L2-регуляризация (Ridge):**\n",
        "$$\n",
        "L(w) = \\lambda_2 \\sum_{j=1}^{p} w_j^2\n",
        "$$\n",
        "- **ElasticNet** (комбинация L1 и L2):\n",
        "$$\n",
        "L(w) = \\lambda_1 \\sum_{j=1}^{p} |w_j| + \\lambda_2 \\sum_{j=1}^{p} w_j^2\n",
        "$$\n",
        "\n",
        "#### 3. **Обновление весов**\n",
        "\n",
        "Обучение модели основано на методе **градиентного спуска**, который итеративно обновляет веса $w$ и смещение $b$.\n",
        "\n",
        "На каждой итерации градиент потерь по весам вычисляется как:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_j} = \\frac{2}{n} \\sum_{i=1}^{n} (X_i \\cdot w + b - y_i) \\cdot X_{ij}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b} = \\frac{2}{n} \\sum_{i=1}^{n} (X_i \\cdot w + b - y_i)\n",
        "$$\n",
        "\n",
        "Обновление весов происходит с использованием правила:\n",
        "\n",
        "$$\n",
        "w_j = w_j - \\eta \\cdot \\frac{\\partial L}{\\partial w_j}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $\\eta$ — это коэффициент обучения (learning rate).\n",
        "\n",
        "#### 4. **Мини-батчи (SGD)**\n",
        "\n",
        "Если указана опция `sgd_sample`, используется стохастический градиентный спуск (SGD). Вместо обновления весов на основе всех данных, используется случайная выборка, которая ускоряет обучение:\n",
        "\n",
        "$$\n",
        "w_j = w_j - \\eta \\cdot \\frac{\\partial L_{mini-batch}}{\\partial w_j}\n",
        "$$\n",
        "\n",
        "### Пример работы на простых данных\n",
        "\n",
        "#### 1. **Инициализация данных**\n",
        "\n",
        "Пусть у нас есть маленькая матрица \\( X \\) с 3 примерами и 2 признаками, а также вектор целевой переменной \\( y \\).\n",
        "\n",
        "```python\n",
        "X = pd.DataFrame({\n",
        "    'x1': [1, 2, 3],\n",
        "    'x2': [4, 5, 6]\n",
        "})\n",
        "y = pd.Series([7, 8, 9])\n",
        "```\n",
        "\n",
        "Эти данные выглядят так:\n",
        "\n",
        "$$\n",
        "X = \\begin{pmatrix}\n",
        "1 & 4 \\\\\n",
        "2 & 5 \\\\\n",
        "3 & 6 \\\\\n",
        "\\end{pmatrix}, \\quad y = \\begin{pmatrix}\n",
        "7 \\\\\n",
        "8 \\\\\n",
        "9 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "#### 2. **Подготовка и инициализация**\n",
        "\n",
        "При вызове метода `fit`, на первом этапе к матрице \\( X \\) добавляется столбец с единицами, чтобы учесть смещение (bias):\n",
        "\n",
        "$$\n",
        "X' = \\begin{pmatrix}\n",
        "1 & 1 & 4 \\\\\n",
        "1 & 2 & 5 \\\\\n",
        "1 & 3 & 6 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "Инициализируем веса **единицами**:\n",
        "\n",
        "$$\n",
        "w = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "#### 3. **Первый шаг обучения**\n",
        "\n",
        "Теперь вычислим предсказанные значения:\n",
        "\n",
        "$$\n",
        "\\hat{y} = X' \\cdot w = \\begin{pmatrix}\n",
        "1 & 1 & 4 \\\\\n",
        "1 & 2 & 5 \\\\\n",
        "1 & 3 & 6 \\\\\n",
        "\\end{pmatrix} \\cdot \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 8 \\\\ 10 \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Вычислим ошибку (разницу между предсказанными и реальными значениями):\n",
        "\n",
        "$$\n",
        "\\text{error} = \\hat{y} - y = \\begin{pmatrix} 6 \\\\ 8 \\\\ 10 \\end{pmatrix} - \\begin{pmatrix} 7 \\\\ 8 \\\\ 9 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Градиент для весов (без регуляризации):\n",
        "\n",
        "$$\n",
        "\\text{gradient} = \\frac{2}{n} X'^T \\cdot (\\hat{y} - y) = \\frac{2}{3} \\begin{pmatrix}\n",
        "1 & 1 & 1 \\\\\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "\\end{pmatrix} \\cdot \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0.6667 \\\\ 0.6667 \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Теперь обновляем веса, используя шаг обучения $\\eta = 0.1$:\n",
        "\n",
        "$$\n",
        "w = w - \\eta \\cdot \\text{gradient} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - 0.1 \\cdot \\begin{pmatrix} 0 \\\\ 0.6667 \\\\ 0.6667 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0.93333 \\\\ 0.93333 \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "#### 4. **Последующие шаги**\n",
        "\n",
        "Этот процесс повторяется для заданного количества итераций \\( n\\_iter \\), пока веса не сойдутся к оптимальным значениям.\n",
        "\n",
        "### Заключение\n",
        "\n",
        "Метод `fit` вашего класса реализует градиентный спуск для минимизации функции потерь, учитывая возможную регуляризацию. Важные моменты:\n",
        "- добавление единичного столбца для смещения (bias),\n",
        "- вычисление градиентов и обновление весов,\n",
        "- опция для стохастического градиентного спуска с использованием мини-батчей.\n",
        "\n",
        "На простых данных можно видеть, как веса обновляются на основе градиента, постепенно приближаясь к оптимальным значениям."
      ],
      "metadata": {
        "id": "ZdzFLiuhOOt_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ll13bxKAOtBx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}